{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64340e-ea68-4791-864f-e7279cfd9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solving bipedal walker using PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe9e6ba-3ee6-4d25-9ede-6983cf12f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install swig\n",
    "!pip install gymnasium['box2d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "872c1ca6-42b9-4f11-b792-c875278d4aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a70916-739f-4152-ae25-217d72b9ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1bd978-d7df-4ff2-83a1-754e38bd3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "# enable interactive mode\n",
    "plt.ion()\n",
    "\n",
    "# setup the device to be used\n",
    "# all the tensors are to be sent to this device for rendering and manipulation\n",
    "# in this case video rendering\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99172c4e-58a9-475c-91dc-c45039d3a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Hyperparameters\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "# Learning rate for the optimizer (e.g., Adam optimizer).\n",
    "# - High values (e.g., > 0.01): Faster updates, but may lead to instability or divergence.\n",
    "# - Low values (e.g., < 0.0001): Slower updates, may take longer to converge but often more stable.\n",
    "\n",
    "GAMMA = 0.99\n",
    "# Discount factor for future rewards in the return calculation.\n",
    "# - High values (close to 1, e.g., 0.99): The agent places more importance on long-term rewards.\n",
    "# - Low values (close to 0, e.g., 0.8): The agent focuses more on immediate rewards, which can hinder performance in environments requiring long-term planning.\n",
    "\n",
    "CLIP_EPSILON = 0.1\n",
    "# Clipping range for the probability ratio in the PPO loss function.\n",
    "# - High values (e.g., 0.4): Allows larger updates, which can lead to faster learning but risks instability.\n",
    "# - Low values (e.g., 0.1): Constrains updates to be more conservative, improving stability but potentially slowing learning.\n",
    "\n",
    "ENTROPY_BETA = 0.01\n",
    "# Coefficient for the entropy bonus in the PPO loss function.\n",
    "# - High values (e.g., 0.05): Increases exploration by encouraging more diverse actions, which is useful in highly stochastic environments but may slow convergence.\n",
    "# - Low values (e.g., 0.001): Reduces exploration, focusing on exploitation of the current policy, which may lead to premature convergence to suboptimal policies.\n",
    "\n",
    "EPOCHS = 3\n",
    "# Number of passes over the collected data during each PPO update.\n",
    "# - High values (e.g., 10): Allows more optimization for each batch, but may lead to overfitting the sampled data.\n",
    "# - Low values (e.g., 1): Reduces the risk of overfitting but may under-optimize the policy during updates.\n",
    "\n",
    "STEPS_PER_UPDATE = 2048\n",
    "# Number of environment steps collected before performing a PPO update.\n",
    "# - High values (e.g., 5000): Collects more diverse data per update, improving training stability but increasing memory and computational requirements.\n",
    "# - Low values (e.g., 512): Faster updates but with less diverse data, which can lead to noisier updates and reduced performance.\n",
    "\n",
    "MAX_TIMESTEPS = 200000\n",
    "# Total number of timesteps to train the agent.\n",
    "# - High values (e.g., > 1,000,000): Allows more time for training, which is essential for complex tasks but increases training time.\n",
    "# - Low values (e.g., 50,000): Trains faster but risks insufficient exploration and learning.\n",
    "\n",
    "HIDDEN_SIZE = 256\n",
    "# Number of neurons in the hidden layers of the neural network.\n",
    "# - High values (e.g., 256): Increases the model's capacity to learn complex patterns but may lead to overfitting and slower computation.\n",
    "# - Low values (e.g., 16): Reduces model capacity, which may result in underfitting and failure to capture environment dynamics.\n",
    "\n",
    "LAM = 0.8\n",
    "# Lambda parameter for Generalized Advantage Estimation (GAE).\n",
    "# - High values (close to 1, e.g., 0.98): Reduces variance in advantage estimates but introduces more bias.\n",
    "# - Low values (close to 0, e.g., 0.9): Increases variance in advantage estimates but reduces bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a44126a8-388a-41b3-8394-cfda54ad710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"BipedalWalker-v3\", hardcore=False, render_mode=\"human\")\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\", continuous=True)\n",
    "# env = gym.make(\"InvertedDoublePendulum-v4\", render_mode=\"human\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca9be043-85b2-4c25-8b0e-b23dcf7840b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Actor-Critic Neural Network for PPO.\n",
    "\n",
    "    This class represents a combined actor-critic model:\n",
    "    - The shared layers process the input state to extract features.\n",
    "    - The actor network outputs the mean and standard deviation of a Gaussian distribution for the actions.\n",
    "    - The critic network outputs the estimated value of the input state.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    state_dim : int\n",
    "        The dimension of the input state space.\n",
    "    action_dim : int\n",
    "        The dimension of the action space.\n",
    "    hidden_size : int\n",
    "        The number of neurons in the hidden layers.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    mean : torch.Tensor\n",
    "        The mean of the Gaussian distribution for the actions. Shape: [batch_size, action_dim]\n",
    "    std : torch.Tensor\n",
    "        The standard deviation of the Gaussian distribution for the actions. Shape: [batch_size, action_dim]\n",
    "    value : torch.Tensor\n",
    "        The estimated value of the input state. Shape: [batch_size, 1]\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_size):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # Shared feature extractor (simplified structure)\n",
    "        self.layer1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        # Actor-specific layers\n",
    "        self.actor_mean = nn.Linear(hidden_size, action_dim)\n",
    "        self.actor_std = nn.Linear(hidden_size, action_dim)\n",
    "\n",
    "        # Critic-specific layer\n",
    "        self.critic = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass through the shared, actor, and critic networks.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        state : torch.Tensor\n",
    "            The input state tensor. Shape: [batch_size, state_dim]\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        mean : torch.Tensor\n",
    "            The mean of the Gaussian distribution for the actions. Shape: [batch_size, action_dim]\n",
    "        std : torch.Tensor\n",
    "            The standard deviation of the Gaussian distribution for the actions. Shape: [batch_size, action_dim]\n",
    "        value : torch.Tensor\n",
    "            The estimated value of the input state. Shape: [batch_size, 1]\n",
    "        \"\"\"\n",
    "        # Shared layers\n",
    "        x = F.relu(self.layer1(state))\n",
    "        x = F.relu(self.layer2(x))\n",
    "\n",
    "        # Actor outputs\n",
    "        mean = torch.tanh(self.actor_mean(x))  # Mean of actions\n",
    "        std = torch.clamp(torch.exp(self.actor_std(x)), 1e-3, 1.0)  # Standard deviation\n",
    "\n",
    "        # Critic output\n",
    "        value = self.critic(x)  # State value\n",
    "\n",
    "        return mean, std, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d481d19-0d1a-4840-890e-dae67a15a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model and optimizer\n",
    "model = ActorCritic(state_dim, action_dim, HIDDEN_SIZE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec007782-f4e9-4e55-949e-7c5b82e22bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Memory to store trajectory data\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.log_probs.clear()\n",
    "        self.rewards.clear()\n",
    "        self.dones.clear()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "057cb566-0b8a-4634-a843-7223a609d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory()\n",
    "\n",
    "# Compute discounted rewards and advantages\n",
    "def compute_gae(next_value, rewards, dones, values, gamma, lam):\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation (GAE).\n",
    "    Args:\n",
    "        next_value: Value of the next state (scalar).\n",
    "        rewards: List of rewards.\n",
    "        dones: List of done flags.\n",
    "        values: List of value predictions (scalars).\n",
    "        gamma: Discount factor.\n",
    "        lam: GAE lambda.\n",
    "\n",
    "    Returns:\n",
    "        List of discounted rewards (returns).\n",
    "    \"\"\"\n",
    "    values = values + [next_value]\n",
    "    gae = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * (1 - dones[step]) - values[step]\n",
    "        gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "        returns.insert(0, gae + values[step])\n",
    "        print(f\"Step {step}: Reward={rewards[step]}, Value={values[step]}, Delta={delta}\")\n",
    "\n",
    "    return returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdda4a0c-27d3-4cf0-a2c2-414aede15478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to plot the episode duration to monitor learning progress\n",
    "episode_rewards = []\n",
    "def plot_rewards(show_result=False):\n",
    "    \"\"\"\n",
    "    Plots the rewards for each episode and the moving average of the last 100 episodes.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    show_result : bool\n",
    "        If True, show the final plot at the end of training.\n",
    "    \"\"\"\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)  # Convert rewards to tensor\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.title('Training')\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(rewards_t.numpy(), label='Episode Rewards')\n",
    "\n",
    "    # Plot moving average of the last 100 episodes\n",
    "    if len(rewards_t) >= 100:\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1)  # 100-episode moving average\n",
    "        means = torch.cat((torch.zeros(99), means))  # Pad to align with episodes\n",
    "        plt.plot(means.numpy(), label='100-Episode Reward Average', linestyle='--')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.pause(0.001)  # Pause to update the plot in real-time\n",
    "\n",
    "    # Display in Jupyter/IPython\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d252fbbc-81e6-4e96-b374-250b275e7f97",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Step the environment\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Track cumulative rewards\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gymnasium/envs/box2d/lunar_lander.py:665\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    662\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 665\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gymnasium/envs/box2d/lunar_lander.py:779\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 779\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    782\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    783\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timesteps = 0\n",
    "while timesteps < MAX_TIMESTEPS:\n",
    "    state = env.reset()\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]\n",
    "    memory.clear()\n",
    "    critic_values = []  # Separate list for critic values\n",
    "    cumulative_reward = 0  # Track cumulative reward for the episode\n",
    "\n",
    "    # Collect trajectories\n",
    "    for _ in range(STEPS_PER_UPDATE):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)  # [1, state_dim]\n",
    "        mean, std, value = model(state_tensor)\n",
    "        dist = Normal(mean, std)\n",
    "        action = torch.tanh(dist.sample())\n",
    "        log_prob = dist.log_prob(action).sum(axis=-1)\n",
    "\n",
    "        # Step the environment\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.squeeze(0).detach().numpy())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # Track cumulative rewards\n",
    "        cumulative_reward += reward\n",
    "\n",
    "        # Store trajectory data\n",
    "        memory.states.append(state_tensor.detach())  # Store state tensors\n",
    "        memory.actions.append(action.detach())\n",
    "        memory.log_probs.append(log_prob.detach())\n",
    "        memory.rewards.append(reward)\n",
    "        memory.dones.append(done)\n",
    "        critic_values.append(value.squeeze().item())  # Store critic value scalars separately\n",
    "\n",
    "        state = next_state\n",
    "        timesteps += 1\n",
    "\n",
    "        if done:\n",
    "            # Log cumulative reward for the completed episode\n",
    "            episode_rewards.append(cumulative_reward)\n",
    "            plot_rewards()  # Update the plot in real-time\n",
    "\n",
    "            # Reset cumulative reward for the next episode\n",
    "            cumulative_reward = 0\n",
    "\n",
    "            # Reset environment if episode ends\n",
    "            state = env.reset()\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "\n",
    "        if timesteps >= MAX_TIMESTEPS:\n",
    "            break\n",
    "\n",
    "    # Compute advantages and returns\n",
    "    next_value = 0 if done else model(torch.FloatTensor(state).unsqueeze(0))[2].item()\n",
    "    returns = compute_gae(\n",
    "        next_value,\n",
    "        memory.rewards,\n",
    "        memory.dones,\n",
    "        critic_values,\n",
    "        GAMMA,\n",
    "        LAM\n",
    "    )\n",
    "    advantages = np.array([ret - val for ret, val in zip(returns, critic_values)])\n",
    "    advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-8)\n",
    "\n",
    "    # Convert data to tensors\n",
    "    states = torch.cat(memory.states)  # Concatenate full state tensors\n",
    "    actions = torch.cat(memory.actions)\n",
    "    log_probs = torch.cat(memory.log_probs)\n",
    "    returns = torch.FloatTensor(returns).detach()\n",
    "    advantages = torch.FloatTensor(advantages).detach()\n",
    "\n",
    "    # PPO updates\n",
    "    for _ in range(EPOCHS):\n",
    "        # Get new action probabilities and values\n",
    "        mean, std, values = model(states)\n",
    "        dist = Normal(mean, std)\n",
    "        new_log_probs = dist.log_prob(actions).sum(axis=-1)\n",
    "\n",
    "        # Probability ratio\n",
    "        ratio = torch.exp(new_log_probs - log_probs.detach())\n",
    "\n",
    "        # Clipped loss\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - CLIP_EPSILON, 1 + CLIP_EPSILON) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "        # Value loss\n",
    "        value_loss = nn.MSELoss()(values.squeeze(), returns)\n",
    "\n",
    "        # Entropy bonus\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        # Total loss\n",
    "        loss = policy_loss + 0.5 * value_loss - ENTROPY_BETA * entropy\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Log progress\n",
    "    print(f\"Timesteps: {timesteps}, Loss: {loss.item()}\")\n",
    "\n",
    "env.close()\n",
    "print('Training Complete')\n",
    "plot_rewards(show_result=True)  # Show the final plot\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899b5ef8-d8ff-4ea5-b9df-a6cd54f110ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "action.squeeze(0).detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896d9f33-adba-4be1-9fec-b325959771cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "for _ in range(100):\n",
    "    # this is where you would insert your policy\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b643205-370e-4e4c-bd89-64383bb59340",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a116ed-9da2-40c6-b3ea-a613eb2dbb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Critic values mean: {np.mean(critic_values)}, std: {np.std(critic_values)}\")\n",
    "print(f\"Episode rewards mean: {np.mean(episode_rewards)}, std: {np.std(episode_rewards)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f8e79d-6b6d-4f3d-b070-a8a66a11ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loss: {loss.item()}, Policy Loss: {policy_loss.item()}, Value Loss: {value_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a73ef04-14ba-450a-a63f-df192235e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Advantages mean: {advantages.numpy().mean()}, std: {advantages.numpy().std()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad344ba5-0347-48b5-b8e5-b1e2872cd7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random.sample(memory.rewards, 10))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f9fbb-1c1b-4557-bfc6-94fa27f502a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Entropy: {entropy.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f80c2-7463-4182-b88e-1f8ad2778e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "the reward is established with the environment and should be OK for PPO. Anyway it is OK when I train the lunar lander using DQN. I should not touch the reward.\n",
    "the model:\n",
    "ActorCritic(\n",
    "  (layer1): Linear(in_features=8, out_features=256, bias=True)\n",
    "  (layer2): Linear(in_features=256, out_features=256, bias=True)\n",
    "  (actor_mean): Linear(in_features=256, out_features=2, bias=True)\n",
    "  (actor_std): Linear(in_features=256, out_features=2, bias=True)\n",
    "  (critic): Linear(in_features=256, out_features=1, bias=True)\n",
    ")\n",
    "How to check if the network is not properly initialized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e000fa3-4ed7-4113-9193-b9a125d49217",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.var(torch.cat(memory.actions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4956f9cb-e94e-495b-af05-e4fded3f3111",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Returns mean: {returns.numpy().mean()}, Critic Values mean: {np.mean(critic_values)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d7910-5c5a-45c9-b72b-88944fcb2a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tanh(dist.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb1b7a-4156-485e-92bb-44458deadbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " 2045: Reward=0.2979755661431485, Value=-0.003003731369972229, Delta=0.2964518185499841\n",
    "Step 2044: Reward=0.38367845851512034, Value=0.0006709322333335876, Delta=0.38003383222551423\n",
    "Step 2043: Reward=0.27197683146362817, Value=-0.0011911019682884216, Delta=0.27383215634291685\n",
    "Step 2042: Reward=0.3942888655563048, Value=-0.0008895173668861389, Delta=0.39399919197458544\n",
    "Step 2041: Reward=0.3404212006632272, Value=0.001573670655488968, Delta=0.33796690781452093\n",
    "Step 2040: Reward=0.28639629020577567, Value=0.002844579517841339, Delta=0.2851096446368684\n",
    "Step 2039: Reward=0.3488555654650009, Value=0.005282096564769745, Delta=0.3463896026228941\n",
    "Step 2038: Reward=0.3266526474470416, Value=0.007256411015987396, Delta=0.32462551203017626"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
