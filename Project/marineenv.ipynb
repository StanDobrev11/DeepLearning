{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4af1cab-73ee-458d-8ae0-ca050c98ffa2",
   "metadata": {},
   "source": [
    "# Creating Marine Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181518bf28e85ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.registration import register, registry\n",
    "import numpy as np\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd77b65221fa08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'MarineEnv-v0' not in registry:\n",
    "    register(\n",
    "        id='MarineEnv-v0',\n",
    "        entry_point='marine_env:MarineEnv',  # String reference to the class\n",
    "    )\n",
    "\n",
    "# Test the environment\n",
    "env = gym.make('MarineEnv-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc0218f078f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83dfb28b4b057409",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(typename='Transition', field_names=('state', 'action', 'next_state', 'reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd116a08b48339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    A replay buffer that stores transitions encountered by the agent.\n",
    "    It enables experience replay by sampling random batches of transitions\n",
    "    to break temporal correlation and improve learning stability.\n",
    "\n",
    "    Attributes:\n",
    "        capacity (int): The maximum number of transitions to store in memory.\n",
    "        memory (deque): A double-ended queue to store transitions with fixed size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initializes the ReplayMemory with a fixed capacity.\n",
    "\n",
    "        Args:\n",
    "            capacity (int): The maximum number of transitions the buffer can hold.\n",
    "        \"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        Add a transition to the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            *args: The elements of a transition (state, action, reward, next_state, done),\n",
    "                   which will be wrapped into a Transition namedtuple.\n",
    "        \"\"\"\n",
    "        state, action, next_state, reward = args\n",
    "        state = state.to(device)\n",
    "        action = action.to(device)\n",
    "        if next_state is not None:\n",
    "            next_state = next_state.to(device)\n",
    "        reward = reward.to(device)\n",
    "\n",
    "        self.memory.append(Transition(state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of transitions from the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): The number of transitions to sample.\n",
    "\n",
    "        Returns:\n",
    "            List[Transition]: A list of randomly sampled transitions.\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the current size of the memory buffer.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of transitions currently stored in memory.\n",
    "        \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f67ce5fdf5e23596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Deep Q-Network (DQN) implemented as a Multilayer Perceptron (MLP).\n",
    "\n",
    "    The network approximates the Q-value function, which predicts the cumulative \n",
    "    reward for each possible action in a given state.\n",
    "\n",
    "    Attributes:\n",
    "        layer1 (nn.Linear): First fully connected layer (input to 128 neurons).\n",
    "        layer2 (nn.Linear): Second fully connected layer (128 to 128 neurons).\n",
    "        layer3 (nn.Linear): Output layer (128 to n_actions neurons).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int):\n",
    "        \"\"\"\n",
    "        Initializes the DQN network with three fully connected layers.\n",
    "\n",
    "        Args:\n",
    "            n_observations (int): The size of the input (state of the environment).\n",
    "            n_actions (int): The number of possible actions in the environment.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)  # Input to hidden layer 1\n",
    "        self.layer2 = nn.Linear(128, 128)  # Hidden layer 1 to hidden layer 2\n",
    "        self.layer3 = nn.Linear(128, n_actions)  # Hidden layer 2 to output\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor representing the state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The Q-values for each possible action.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.layer1(x))  # Apply ReLU to the first layer\n",
    "        x = F.relu(self.layer2(x))  # Apply ReLU to the second layer\n",
    "        return self.layer3(x)  # Output layer (no activation applied)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0d9e53f75699072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the batch sampled from the replay buffer\n",
    "BATCH_SIZE = 128\n",
    "# Explanation:\n",
    "# The number of experiences (state-action-reward transitions) sampled from the replay buffer\n",
    "# at each training step. Larger batch sizes improve stability but increase computational cost.\n",
    "\n",
    "# Discount factor for future rewards\n",
    "GAMMA = 0.99\n",
    "# Explanation:\n",
    "# The discount factor determines how much importance is given to future rewards.\n",
    "# A value close to 1 means future rewards are highly valued, while a value closer to 0 means\n",
    "# the agent focuses on immediate rewards.\n",
    "\n",
    "# Initial value for epsilon in the epsilon-greedy policy\n",
    "EPS_START = 0.9\n",
    "# Explanation:\n",
    "# Epsilon is the probability of choosing a random action (exploration).\n",
    "# This is the starting value, meaning the agent initially explores 90% of the time.\n",
    "\n",
    "# Final value for epsilon in the epsilon-greedy policy\n",
    "EPS_END = 0.05\n",
    "# Explanation:\n",
    "# The minimum value epsilon can decay to. At this stage, the agent mostly exploits\n",
    "# the learned policy but still explores 5% of the time to avoid getting stuck in local optima.\n",
    "\n",
    "# Decay rate for epsilon in the epsilon-greedy policy\n",
    "EPS_DECAY = 1000\n",
    "# Explanation:\n",
    "# The rate at which epsilon decays over time. A higher value makes epsilon decay slower,\n",
    "# allowing the agent to explore for longer. The effective epsilon at time step `t` can\n",
    "# be calculated as:\n",
    "# epsilon(t) = EPS_END + (EPS_START - EPS_END) * exp(-t / EPS_DECAY)\n",
    "\n",
    "# Target network soft update rate\n",
    "TAU = 0.005\n",
    "# Explanation:\n",
    "# Determines how much the target network is updated towards the online (policy) network\n",
    "# at each training step. A lower TAU makes the target network update more slowly,\n",
    "# stabilizing the learning process. The update rule is:\n",
    "# target_param = TAU * policy_param + (1 - TAU) * target_param\n",
    "\n",
    "# Learning rate for the optimizer\n",
    "LR = 1e-04\n",
    "# Explanation:\n",
    "# The step size for gradient updates during backpropagation.\n",
    "# A smaller learning rate ensures slow and stable learning but may require more steps to converge.\n",
    "# Too large a value may lead to instability in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb0a4cfd21d8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of actions in the environment\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "# Initialize the policy and target networks\n",
    "policy_net = DQN(n_observations=n_observations, n_actions=n_actions).to(device)\n",
    "target_net = DQN(n_observations=n_observations, n_actions=n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())  # Copy weights from policy_net to target_net\n",
    "\n",
    "# Optimizer for training the policy network\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR)\n",
    "\n",
    "# Replay memory for experience replay\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# Step counter for epsilon decay\n",
    "steps_done = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eafee99f5da85578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Selects an action using an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        state (torch.Tensor): The current state of the environment.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The selected action as a 1x1 tensor (contains the action index).\n",
    "    \"\"\"\n",
    "    global steps_done\n",
    "    sample = random.random()  # Random number to determine explore vs exploit\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1.0 * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample > eps_threshold:  # Exploit: Use policy network to select the best action\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1).to(device)\n",
    "    else:  # Explore: Select a random action\n",
    "        return torch.tensor([[env.action_space.sample()]], dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8299abfed6125eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards = []  # To store cumulative rewards per episode\n",
    "\n",
    "\n",
    "def plot_rewards(show_result=False):\n",
    "    plt.figure(1)\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training')\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.plot(rewards_t.numpy(), label='Episode Reward')\n",
    "\n",
    "    # Plot moving average of the last 100 episodes\n",
    "    if len(rewards_t) >= 100:\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy(), label='100-Episode Average')\n",
    "\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58f23ad34b274568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \"\"\"\n",
    "    Perform one step of optimization for the policy network using the replay memory.\n",
    "\n",
    "    The function:\n",
    "        - Samples a batch of transitions from the replay memory.\n",
    "        - Computes the expected Q-values for the current state-action pairs.\n",
    "        - Computes the loss between the expected Q-values and the predicted Q-values.\n",
    "        - Performs a backward pass to update the policy network's parameters.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return  # Exit if there are not enough samples in memory to form a batch\n",
    "\n",
    "    # Sample a batch of transitions from the replay memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))  # Convert batch-array of transitions to Transition of batch-arrays\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool\n",
    "    )\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model's Q-values for the current states and actions\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Initialize tensor for the next state values\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    # Compute expected Q-values\n",
    "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
    "\n",
    "    # Compute Huber loss (Smooth L1 Loss)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()  # Clear the gradients from the previous step\n",
    "    loss.backward()  # Backpropagate the loss\n",
    "\n",
    "    # Clip gradients to avoid exploding gradients\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), clip_value=1.0)\n",
    "\n",
    "    # Perform a single optimization step\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de9d254b348fa70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update_target_network(policy_net, target_net, tau=0.01):\n",
    "    \"\"\"\n",
    "    Perform a soft update of the target network parameters.\n",
    "\n",
    "    Args:\n",
    "        policy_net (nn.Module): The policy network whose parameters are used for the update.\n",
    "        target_net (nn.Module): The target network to be updated.\n",
    "        tau (float): The soft update coefficient. Values closer to 1.0 mean faster updates.\n",
    "    \"\"\"\n",
    "    for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "        target_param.data.copy_(tau * policy_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e431ff27c95df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "training = True\n",
    "\n",
    "if training:\n",
    "    env.unwrapped.time_scale = 10\n",
    "    env.unwrapped.training = True\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    rewards = 0\n",
    "    \n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        rewards += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # move to next state\n",
    "        state = next_state\n",
    "\n",
    "        # Update the model every N steps\n",
    "        # TRAIN_EVERY = 4\n",
    "        # if steps_done % TRAIN_EVERY == 0:\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update the target network\n",
    "        soft_update_target_network(policy_net, target_net, tau=TAU)\n",
    "\n",
    "        if done:\n",
    "            episode_rewards.append(rewards)\n",
    "            plot_rewards()\n",
    "            break\n",
    "\n",
    "print('Completed')\n",
    "plot_rewards(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0f19a-48cf-44fb-a24f-6ed224bc5363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(policy_net.state_dict(), 'dqn_policy_1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17507e72-2558-4040-87c2-a32dbbaa7813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61/1047014131.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  policy_net.load_state_dict(torch.load(\"dqn_policy_1.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net.load_state_dict(torch.load(\"dqn_policy_1.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35aeb27cf672e3f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'fill'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Render the environment\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Prepare the next state\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py:409\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is an intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m     )\n\u001b[0;32m--> 409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/core.py:332\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RenderFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[RenderFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`render` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py:303\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_render_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/Project/marine_env.py:470\u001b[0m, in \u001b[0;36mMarineEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    469\u001b[0m \u001b[38;5;66;03m# Clear the screen\u001b[39;00m\n\u001b[0;32m--> 470\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m))  \u001b[38;5;66;03m# Dark blue background\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# Draw the own ship\u001b[39;00m\n\u001b[1;32m    473\u001b[0m lat, lon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mown_ship\u001b[38;5;241m.\u001b[39mlat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mown_ship\u001b[38;5;241m.\u001b[39mlon\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'fill'"
     ]
    }
   ],
   "source": [
    "# Environment setup\n",
    "env = gym.make('MarineEnv-v0')\n",
    "\n",
    "# Reset the environment\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "for t in range(10000):  # Maximum steps for visualization\n",
    "    with torch.no_grad():\n",
    "        # Use the trained policy network to select the best action\n",
    "        action = policy_net(state).max(1)[1].view(1, 1).item()\n",
    "        # action = env.action_space.sample()\n",
    "\n",
    "    # Take the action in the environment\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Render the environment\n",
    "    env.render()\n",
    "\n",
    "    # Prepare the next state\n",
    "    if not terminated:\n",
    "        state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    else:\n",
    "        break\n",
    "print(f\"Simulated Time: {info['total_sim_time']} hours\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b9d1407-13eb-4efc-981e-ac26d528e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd8c7f-db86-46f4-ad3b-c6197b9f4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dda22f-5fcf-442e-9eff-81d23d1e21d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11ab12-d065-4592-b59d-f1a723d8f6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.target_ship.cpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a32c7c-1426-4e3c-b980-9ab0c2b9683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926f41e-6021-42d8-bbcd-02980cb7e6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0d12f24-0ff2-42bb-b9cc-d0b9b146f3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-14 21:33:06.475465: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-14 21:33:06.639978: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b9dd6ab-17b0-443a-9c86-52968710ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_env = gym.make('MarineEnv-v0')\n",
    "\n",
    "training = True\n",
    "\n",
    "if training:\n",
    "    sb_env.unwrapped.time_scale = 10\n",
    "    sb_env.unwrapped.training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a68f8aed-9e44-41a1-bab0-28eeb7795ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    sb_env,\n",
    "    verbose=1,\n",
    "    exploration_final_eps=0.1,\n",
    "    target_update_interval=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6acd376f-d60e-4ce0-b901-010550f0f55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=9423.80 +/- 17082.779345248404\n"
     ]
    }
   ],
   "source": [
    "# Separate env for evaluation\n",
    "eval_env = gym.make(\"MarineEnv-v0\")\n",
    "eval_env.unwrapped.time_scale *= 10\n",
    "eval_env.unwrapped.training = True\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "820814a1-5ab1-4d43-b9e9-5e345edb6011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27d4c3c9b1b41e794a927ae095073c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 2.7e+04   |\n",
      "|    ep_rew_mean      | -2.52e+03 |\n",
      "|    exploration_rate | 0.1       |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 544       |\n",
      "|    time_elapsed     | 198       |\n",
      "|    total_timesteps  | 107941    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 4.71      |\n",
      "|    n_updates        | 26960     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7e+04  |\n",
      "|    ep_rew_mean      | 8.25e+03 |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 535      |\n",
      "|    time_elapsed     | 254      |\n",
      "|    total_timesteps  | 135997   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.31     |\n",
      "|    n_updates        | 33974    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.41e+04 |\n",
      "|    ep_rew_mean      | 1.21e+04 |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 519      |\n",
      "|    time_elapsed     | 325      |\n",
      "|    total_timesteps  | 168919   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.701    |\n",
      "|    n_updates        | 42204    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.23e+04 |\n",
      "|    ep_rew_mean      | 1.44e+04 |\n",
      "|    exploration_rate | 0.1      |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 507      |\n",
      "|    time_elapsed     | 388      |\n",
      "|    total_timesteps  | 197164   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.931    |\n",
      "|    n_updates        | 49265    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f849f6a2190>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(2e5), progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b4e2d87-3f9e-471b-af9b-4959153f4b1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/evaluation.py:94\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m     88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m     89\u001b[0m         observations,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         state\u001b[38;5;241m=\u001b[39mstates,\n\u001b[1;32m     91\u001b[0m         episode_start\u001b[38;5;241m=\u001b[39mepisode_starts,\n\u001b[1;32m     92\u001b[0m         deterministic\u001b[38;5;241m=\u001b[39mdeterministic,\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     96\u001b[0m     current_lengths \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/app/Project/marine_env.py:220\u001b[0m, in \u001b[0;36mMarineEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    213\u001b[0m     lat, lon \u001b[38;5;241m=\u001b[39m plane_sailing_next_position(\n\u001b[1;32m    214\u001b[0m         [target\u001b[38;5;241m.\u001b[39mlat, target\u001b[38;5;241m.\u001b[39mlon],\n\u001b[1;32m    215\u001b[0m         target\u001b[38;5;241m.\u001b[39mcourse,\n\u001b[1;32m    216\u001b[0m         target\u001b[38;5;241m.\u001b[39mspeed,\n\u001b[1;32m    217\u001b[0m         time_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_dt\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m     target\u001b[38;5;241m.\u001b[39mlat, target\u001b[38;5;241m.\u001b[39mlon \u001b[38;5;241m=\u001b[39m lat, lon\n\u001b[0;32m--> 220\u001b[0m     target\u001b[38;5;241m.\u001b[39mcpa, target\u001b[38;5;241m.\u001b[39mtcpa \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mown_ship\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_cpa_tcpa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# extract own ship params from the current state\u001b[39;00m\n\u001b[1;32m    223\u001b[0m course, speed, previous_rel_wp_bearing, previous_wp_distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[0;32m/app/Project/marine_env.py:137\u001b[0m, in \u001b[0;36mOwnShip.calculate_cpa_tcpa\u001b[0;34m(self, target_ship)\u001b[0m\n\u001b[1;32m    134\u001b[0m ry \u001b[38;5;241m=\u001b[39m (target_ship\u001b[38;5;241m.\u001b[39mlat \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlat) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m  \u001b[38;5;66;03m# Latitudinal distance (NM)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Relative velocity magnitude squared\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m v_rel_squared \u001b[38;5;241m=\u001b[39m rel_vx \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mrel_vy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# Dot product of relative position and velocity\u001b[39;00m\n\u001b[1;32m    140\u001b[0m r_dot_v_rel \u001b[38;5;241m=\u001b[39m rx \u001b[38;5;241m*\u001b[39m rel_vx \u001b[38;5;241m+\u001b[39m ry \u001b[38;5;241m*\u001b[39m rel_vy\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab10cf57-6e28-466b-a1bc-8b42a012213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "vec_env.unwrapped.time_scale = 1\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b7c98ed-981e-45e8-9169-b912b23cd9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[93.19180723,  7.95600933,  2.43173144, 13.53242248]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "433dfff3-55ec-4cf7-947a-30643d000fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(obs, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c42d00e4-ca9b-4ef0-a32f-2d0bcf11374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, dones, info = vec_env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d90f955-5cb8-401c-a5d3-c0cc9ff73d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.272057], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e35cd6-64d3-46e6-ade0-2bd01003d6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
