{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8f13dba-f88e-4a44-ab7e-d55161f8af8d",
   "metadata": {},
   "source": [
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fdee756-e4f4-4867-8c3b-3436773f813b",
   "metadata": {},
   "source": [
    "!pip install gymnasium['classic-control']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfaacd29-7f92-4ba4-bbf6-55a520aef9ec",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5305a03-f277-4b5f-a0a9-f732aaf8923d",
   "metadata": {},
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb72e0d-13c3-4adb-943e-e5847c42e245",
   "metadata": {},
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c342766c-af36-43c9-b52c-9d10d1000292",
   "metadata": {},
   "source": [
    "Transition = namedtuple(typename='Transition', field_names=('state', 'action', 'next_state', 'reward'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "591eb201-1283-4875-825f-2933bf1f7763",
   "metadata": {},
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    A replay buffer that stores transitions encountered by the agent.\n",
    "    It enables experience replay by sampling random batches of transitions\n",
    "    to break temporal correlation and improve learning stability.\n",
    "\n",
    "    Attributes:\n",
    "        capacity (int): The maximum number of transitions to store in memory.\n",
    "        memory (deque): A double-ended queue to store transitions with fixed size.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initializes the ReplayMemory with a fixed capacity.\n",
    "\n",
    "        Args:\n",
    "            capacity (int): The maximum number of transitions the buffer can hold.\n",
    "        \"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        Add a transition to the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            *args: The elements of a transition (state, action, reward, next_state, done),\n",
    "                   which will be wrapped into a Transition namedtuple.\n",
    "        \"\"\"\n",
    "        state, action, next_state, reward = args\n",
    "        state = state.to(device)\n",
    "        action = action.to(device)\n",
    "        if next_state is not None:\n",
    "            next_state = next_state.to(device)\n",
    "        reward = reward.to(device)\n",
    "\n",
    "        self.memory.append(Transition(state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of transitions from the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): The number of transitions to sample.\n",
    "\n",
    "        Returns:\n",
    "            List[Transition]: A list of randomly sampled transitions.\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the current size of the memory buffer.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of transitions currently stored in memory.\n",
    "        \"\"\"\n",
    "        return len(self.memory)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5ea6452-eef4-49da-94bf-d87ace963093",
   "metadata": {},
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Deep Q-Network (DQN) implemented as a Multilayer Perceptron (MLP).\n",
    "\n",
    "    The network approximates the Q-value function, which predicts the cumulative \n",
    "    reward for each possible action in a given state.\n",
    "\n",
    "    Attributes:\n",
    "        layer1 (nn.Linear): First fully connected layer (input to 128 neurons).\n",
    "        layer2 (nn.Linear): Second fully connected layer (128 to 128 neurons).\n",
    "        layer3 (nn.Linear): Output layer (128 to n_actions neurons).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_observations: int, n_actions: int):\n",
    "        \"\"\"\n",
    "        Initializes the DQN network with three fully connected layers.\n",
    "\n",
    "        Args:\n",
    "            n_observations (int): The size of the input (state of the environment).\n",
    "            n_actions (int): The number of possible actions in the environment.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)  # Input to hidden layer 1\n",
    "        self.layer2 = nn.Linear(128, 128)            # Hidden layer 1 to hidden layer 2\n",
    "        self.layer3 = nn.Linear(128, n_actions)      # Hidden layer 2 to output\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor representing the state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The Q-values for each possible action.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.layer1(x))  # Apply ReLU to the first layer\n",
    "        x = F.relu(self.layer2(x))  # Apply ReLU to the second layer\n",
    "        return self.layer3(x)       # Output layer (no activation applied)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a1d37cb-34ed-4c28-a814-8e8c22b7f0a0",
   "metadata": {},
   "source": [
    "# Size of the batch sampled from the replay buffer\n",
    "BATCH_SIZE = 128\n",
    "# Explanation:\n",
    "# The number of experiences (state-action-reward transitions) sampled from the replay buffer\n",
    "# at each training step. Larger batch sizes improve stability but increase computational cost.\n",
    "\n",
    "# Discount factor for future rewards\n",
    "GAMMA = 0.99\n",
    "# Explanation:\n",
    "# The discount factor determines how much importance is given to future rewards.\n",
    "# A value close to 1 means future rewards are highly valued, while a value closer to 0 means\n",
    "# the agent focuses on immediate rewards.\n",
    "\n",
    "# Initial value for epsilon in the epsilon-greedy policy\n",
    "EPS_START = 0.9\n",
    "# Explanation:\n",
    "# Epsilon is the probability of choosing a random action (exploration).\n",
    "# This is the starting value, meaning the agent initially explores 90% of the time.\n",
    "\n",
    "# Final value for epsilon in the epsilon-greedy policy\n",
    "EPS_END = 0.05\n",
    "# Explanation:\n",
    "# The minimum value epsilon can decay to. At this stage, the agent mostly exploits\n",
    "# the learned policy but still explores 5% of the time to avoid getting stuck in local optima.\n",
    "\n",
    "# Decay rate for epsilon in the epsilon-greedy policy\n",
    "EPS_DECAY = 1000\n",
    "# Explanation:\n",
    "# The rate at which epsilon decays over time. A higher value makes epsilon decay slower,\n",
    "# allowing the agent to explore for longer. The effective epsilon at time step `t` can\n",
    "# be calculated as:\n",
    "# epsilon(t) = EPS_END + (EPS_START - EPS_END) * exp(-t / EPS_DECAY)\n",
    "\n",
    "# Target network soft update rate\n",
    "TAU = 0.005\n",
    "# Explanation:\n",
    "# Determines how much the target network is updated towards the online (policy) network\n",
    "# at each training step. A lower TAU makes the target network update more slowly,\n",
    "# stabilizing the learning process. The update rule is:\n",
    "# target_param = TAU * policy_param + (1 - TAU) * target_param\n",
    "\n",
    "# Learning rate for the optimizer\n",
    "LR = 1e-04\n",
    "# Explanation:\n",
    "# The step size for gradient updates during backpropagation.\n",
    "# A smaller learning rate ensures slow and stable learning but may require more steps to converge.\n",
    "# Too large a value may lead to instability in training."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47c1320c-21fa-4c6e-85c8-87fc5af2c852",
   "metadata": {},
   "source": [
    "# Number of actions in the environment\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "# Initialize the policy and target networks\n",
    "policy_net = DQN(n_observations=n_observations, n_actions=n_actions).to(device)\n",
    "target_net = DQN(n_observations=n_observations, n_actions=n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())  # Copy weights from policy_net to target_net\n",
    "\n",
    "# Optimizer for training the policy network\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR)\n",
    "\n",
    "# Replay memory for experience replay\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# Step counter for epsilon decay\n",
    "steps_done = 0\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca3a93d9-f0ea-4af7-aae7-99c1f954affb",
   "metadata": {},
   "source": [
    "def select_action(state: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Selects an action using an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        state (torch.Tensor): The current state of the environment.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The selected action as a 1x1 tensor (contains the action index).\n",
    "    \"\"\"\n",
    "    global steps_done\n",
    "    sample = random.random()  # Random number to determine explore vs exploit\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1.0 * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample > eps_threshold:  # Exploit: Use policy network to select the best action\n",
    "        with torch.no_grad():\n",
    "            # Predict Q-values for the current state and select the action with the highest Q-value\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:  # Explore: Select a random action\n",
    "        return torch.tensor([[env.action_space.sample()]], dtype=torch.long)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e17db374-e65b-4743-ab0a-b6b2e3d3ce7e",
   "metadata": {},
   "source": [
    "# List to store episode durations\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    \"\"\"\n",
    "    Plots the duration of episodes during training or testing.\n",
    "\n",
    "    Args:\n",
    "        show_result (bool): If True, displays the final result. If False, updates the training plot.\n",
    "    \n",
    "    Behavior:\n",
    "        - During training, the plot updates in real-time with episode durations.\n",
    "        - A moving average of the last 100 episodes is also plotted for trend analysis.\n",
    "        - For interactive environments (e.g., Jupyter Notebook), the plot is cleared and refreshed.\n",
    "    \"\"\"\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "\n",
    "    if show_result:\n",
    "        plt.title('Result')  # Title for the final result\n",
    "    else:\n",
    "        plt.clf()  # Clear the current figure to update dynamically\n",
    "        plt.title('Training')  # Title during training\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy(), label='Episode Duration')\n",
    "\n",
    "    # Plot moving average of the last 100 episodes\n",
    "    if len(durations_t) >= 100:\n",
    "        # Compute the moving average\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1)  # 100-episode window\n",
    "        means = torch.cat((torch.zeros(99), means))  # Pad with zeros for alignment\n",
    "        plt.plot(means.numpy(), label='100-Episode Average')\n",
    "\n",
    "    plt.pause(0.001)  # Pause to update the plot in real-time\n",
    "\n",
    "    # Display in Jupyter/IPython\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b8cf294-d565-47a9-8575-762649727df8",
   "metadata": {},
   "source": [
    "def optimize_model():\n",
    "    \"\"\"\n",
    "    Perform one step of optimization for the policy network using the replay memory.\n",
    "\n",
    "    The function:\n",
    "        - Samples a batch of transitions from the replay memory.\n",
    "        - Computes the expected Q-values for the current state-action pairs.\n",
    "        - Computes the loss between the expected Q-values and the predicted Q-values.\n",
    "        - Performs a backward pass to update the policy network's parameters.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return  # Exit if there are not enough samples in memory to form a batch\n",
    "\n",
    "    # Sample a batch of transitions from the replay memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))  # Convert batch-array of transitions to Transition of batch-arrays\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool\n",
    "    )\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model's Q-values for the current states and actions\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Initialize tensor for the next state values\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    # Compute expected Q-values\n",
    "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
    "\n",
    "    # Compute Huber loss (Smooth L1 Loss)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()  # Clear the gradients from the previous step\n",
    "    loss.backward()  # Backpropagate the loss\n",
    "\n",
    "    # Clip gradients to avoid exploding gradients\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), clip_value=1.0)\n",
    "\n",
    "    # Perform a single optimization step\n",
    "    optimizer.step()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "133759db-d343-476a-b4de-94c3bdd88d36",
   "metadata": {},
   "source": [
    "def soft_update_target_network(policy_net, target_net, tau=0.01):\n",
    "    \"\"\"\n",
    "    Perform a soft update of the target network parameters.\n",
    "\n",
    "    Args:\n",
    "        policy_net (nn.Module): The policy network whose parameters are used for the update.\n",
    "        target_net (nn.Module): The target network to be updated.\n",
    "        tau (float): The soft update coefficient. Values closer to 1.0 mean faster updates.\n",
    "    \"\"\"\n",
    "    for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "        target_param.data.copy_(tau * policy_param.data + (1.0 - tau) * target_param.data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1116a8d-eb27-40c8-81ca-6aeb082a2aee",
   "metadata": {},
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "            \n",
    "        # move to next state\n",
    "        state = next_state\n",
    "\n",
    "        # optimize model\n",
    "        optimize_model()\n",
    "\n",
    "        # soft update\n",
    "        soft_update_target_network(policy_net, target_net, tau=TAU)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Completed')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14468206-32e7-4458-906b-0415cc1affbc",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")  # Use 'human' render mode for visualization\n",
    "\n",
    "# Reset the environment\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "# Run the trained policy to balance the pole\n",
    "for t in range(1000):  # Maximum steps for visualization\n",
    "    with torch.no_grad():\n",
    "        # Use the trained policy network to select the best action\n",
    "        action = target_net(state).max(1)[1].view(1, 1).item()\n",
    "\n",
    "    # Take the action in the environment\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Render the environment\n",
    "    env.render()\n",
    "    \n",
    "\n",
    "    # Prepare the next state\n",
    "    if not terminated:\n",
    "        state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    else:\n",
    "        print(f\"Pole balanced for {t+1} steps!\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbd90b6-4df0-4cf9-a087-cced09d58c15",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
