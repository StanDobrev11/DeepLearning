{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c27b3e7-75ca-4d80-81a1-524261fde744",
   "metadata": {},
   "source": [
    "!pip install swig\n",
    "!pip install \"gymnasium[box2d]\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd2d6e4-620c-47f7-bdc4-1b07c92540fc",
   "metadata": {},
   "source": [
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c888d8-2a9e-463c-b271-7c675b662daa",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d9932f6-6c38-4a09-84f1-56a087b11bf0",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import gymnasium as gym"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b28a1f-7e13-4ebc-928d-07b83a8f6103",
   "metadata": {},
   "source": [
    "from IPython.display import clear_output\n",
    "import time"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b4440a-b824-4334-87e2-583784f02894",
   "metadata": {},
   "source": [
    "class BlackjackAgent:\n",
    "    \"\"\"\n",
    "    A Reinforcement Learning agent that uses Q-learning to play the Blackjack game.\n",
    "\n",
    "    The agent learns state-action values (Q-values) over time by interacting with the\n",
    "    environment. It balances exploration and exploitation using an epsilon-greedy policy.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the BlackjackAgent with learning parameters and an empty Q-value table.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): The rate at which the agent learns (alpha in Q-learning).\n",
    "            initial_epsilon (float): The initial exploration rate for epsilon-greedy policy.\n",
    "            epsilon_decay (float): The rate at which epsilon decays after each episode.\n",
    "            final_epsilon (float): The minimum value epsilon can decay to.\n",
    "            discount_factor (float, optional): The discount factor for future rewards (gamma). \n",
    "                                               Defaults to 0.95.\n",
    "        \"\"\"\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: Tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Select an action based on the epsilon-greedy policy.\n",
    "\n",
    "        Args:\n",
    "            obs (Tuple[int, int, bool]): The current state of the environment represented as \n",
    "                                         a tuple (player_sum, dealer_card, usable_ace).\n",
    "\n",
    "        Returns:\n",
    "            int: The chosen action (0 = stick, 1 = hit).\n",
    "        \"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()  # Explore: take a random action\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))  # Exploit: take the best action\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: Tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: Tuple[int, int, bool]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Update the Q-value for a state-action pair using the Q-learning update rule.\n",
    "\n",
    "        Args:\n",
    "            obs (Tuple[int, int, bool]): The current state of the environment.\n",
    "            action (int): The action taken in the current state.\n",
    "            reward (float): The reward received for taking the action.\n",
    "            terminated (bool): Whether the episode has ended after this step.\n",
    "            next_obs (Tuple[int, int, bool]): The resulting state after taking the action.\n",
    "        \"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "        self.q_values[obs][action] += self.lr * temporal_difference\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay the exploration rate epsilon after each episode.\n",
    "\n",
    "        Epsilon decreases multiplicatively until it reaches the minimum value (final_epsilon).\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f289ad7c-3a70-4883-abc6-2aa5b09d6040",
   "metadata": {},
   "source": [
    "Q-value function is used to estimate the optimal action to take in each state. The optimal action in a state is the one that maximazed the long-term reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f608b77-c1bf-446d-9f1f-3f5cc73dff2f",
   "metadata": {},
   "source": [
    "# hyperparams\n",
    "learning_rate = 0.01\n",
    "n_episodes = 1000\n",
    "start_epsilon = 1\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2) # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BlackjackAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f145a27-876a-431d-aebc-f8d441e70d02",
   "metadata": {},
   "source": [
    "env = gym.make('Blackjack-v1', render_mode='rgb_array', sab=True)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=n_episodes)\n",
    "\n",
    "for episode in tqdm(range(n_episodes), desc=\"Training Progress\"):\n",
    "    # clear_output()\n",
    "    episode_finish = False\n",
    "    obs, info = env.reset()  # Reset environment at start of episode\n",
    "\n",
    "    # Run one episode\n",
    "    while not episode_finish:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "        # frame = env.render()\n",
    "        # plt.imshow(frame)\n",
    "        # plt.show()\n",
    "\n",
    "        episode_finish = terminated or truncated\n",
    "        obs = next_obs\n",
    "        sys.stdout.flush()  # Force output to flush for the progress bar\n",
    "\n",
    "    agent.decay_epsilon()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d93d54a8-64e7-489a-ab70-1636e158c3d7",
   "metadata": {},
   "source": [
    "rolling_length = 100\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(12,5))\n",
    "\n",
    "axs[0].set_title('Episode Rewards')\n",
    "reward_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.return_queue).flatten(), np.ones(rolling_length), mode='valid'\n",
    "    ) / rolling_length\n",
    ")\n",
    "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "\n",
    "axs[1].set_title('Episode lengths')\n",
    "length_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(env.length_queue).flatten(), np.ones(rolling_length), mode='same'\n",
    "    ) / rolling_length\n",
    ")\n",
    "axs[1].plot(range(len(length_moving_average)), length_moving_average)\n",
    "\n",
    "\n",
    "axs[2].set_title('Training Error')\n",
    "training_error_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(agent.training_error), np.ones(rolling_length), mode='same'\n",
    "    ) / rolling_length\n",
    ")\n",
    "axs[2].plot(range(len(training_error_moving_average)), training_error_moving_average)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "328bbd45-9c56-416f-b3b6-ef721fa5a97d",
   "metadata": {},
   "source": [
    "**Episode Rewards** helps identify trends in rewards (e.g., increasing rewards show the agent is improving).\n",
    "\n",
    "**Episode Lengths** helps monitor if the agent's episodes are becoming shorter or longer:\n",
    "- Shorter episodes might mean the agent is quickly achieving its goal (or failing fast).\n",
    "- Longer episodes might indicate indecisive behavior or exploration.\n",
    "\n",
    "**Training Error** helps analyze how the error decreases as the agent learns:\n",
    "- High errors: The agent is still learning and updating Q-values aggressively.\n",
    "- Low errors: The agent is converging, and Q-values stabilize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1b18c08-c2cb-4df9-92e2-88d7d7b14d6f",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750cee61-dfab-4038-a6e5-d4dbd385ebac",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
