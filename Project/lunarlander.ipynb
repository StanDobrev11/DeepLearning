{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c89d4c2-d66b-4751-b084-6e70a615c895",
   "metadata": {},
   "source": [
    "# run this command in wsl:\n",
    "\n",
    "# xhost +local:root"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8038aeec-ac9f-480b-9ce6-c00593eaf2e7",
   "metadata": {},
   "source": [
    "!pip install swig\n",
    "!pip install gymnasium['box2d']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4349eb56-f2fd-4fef-9b16-92ffbb1440f7",
   "metadata": {},
   "source": [
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50812c0f-3a50-4090-8f02-27ea9ee58a09",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96731c2e-7673-4679-b3a1-2589f1db37ea",
   "metadata": {},
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698d1eaa-70ee-4c26-b44c-151eb8d7717e",
   "metadata": {},
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "# enable interactive mode\n",
    "plt.ion()\n",
    "\n",
    "# setup the device to be used\n",
    "# all the tensors are to be sent to this device for rendering and manipulation\n",
    "# in this case video rendering\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff2589b-193d-4830-a786-8b31905095c8",
   "metadata": {},
   "source": [
    "device"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da3559-b255-4ecd-8f3c-fbc04da24072",
   "metadata": {},
   "source": [
    "# creating subclass of tuple with named fields\n",
    "# this will be pushed to the replay memory\n",
    "Transition = namedtuple(typename='Transition', field_names=['state', 'action', 'next_state', 'reward'])\n",
    "\n",
    "# creating a class to store the memory to be replayed\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        param: capacity - maximum number of transition the buffer can hold\n",
    "        \"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # all the args ['state', 'action', 'next_state', 'reward'] must be pushed to the tensor's devide before stored in memory\n",
    "        state, action, next_state, reward = args\n",
    "        state = state.to(device)\n",
    "        action = action.to(device)\n",
    "        reward = reward.to(device)\n",
    "        if next_state is not None:\n",
    "            next_state = next_state.to(device)\n",
    "\n",
    "        self.memory.append(Transition(state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # sample the memory buffer\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # current size of the memory buffer\n",
    "        return len(self.memory)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf746dc-886f-4f1a-91d4-83202153bbec",
   "metadata": {},
   "source": [
    "class DQN(nn.Module):\n",
    "    # fully connected NN with 3 layers\n",
    "    # activation function 'RELU'\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)        "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bc57af-9744-4406-8e3a-489e63da991c",
   "metadata": {},
   "source": [
    "# setting hyperparams\n",
    "# the size of the batch for sampling from the memory at each training step\n",
    "# larger size improve stability but increase computational cost\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# starting value of the epsilon\n",
    "EPS_START = 0.9\n",
    "\n",
    "# final value of the epsilon\n",
    "EPS_END = 0.05\n",
    "\n",
    "# decay rate of the epsilon\n",
    "# larger value should stabilize the training\n",
    "EPS_DECAY = 1000\n",
    "\n",
    "# discount coef\n",
    "# values close to 1 favor long term rewards\n",
    "GAMMA = 0.99\n",
    "\n",
    "# determines how mutch target network is updated against policy network\n",
    "TAU = 0.005\n",
    "\n",
    "# learning rate of the agent\n",
    "LR = 1e-04"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1923a-0c23-441c-ae97-2fcbf40a9d1f",
   "metadata": {},
   "source": [
    "n_actions = env.action_space.n\n",
    "\n",
    "n_observations = env.observation_space.shape[0]\n",
    "\n",
    "# initialize the policy and target networks\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "\n",
    "# copy the initial policy network weights to the target network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# setup the optimizer to be used\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR)\n",
    "\n",
    "# setup the memory with certain amount of records (Transitions)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# to keep track of each episode steps for epsilon decay\n",
    "step_count = 0 "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc54904-640d-4b80-ae4f-401417328bde",
   "metadata": {},
   "source": [
    "def take_action(state):\n",
    "    # the function will return the action to be taken from memory buffer or random from the action space based on the epsilon treshold and the decay of epsilon\n",
    "    global step_count\n",
    "\n",
    "    epsilon_treshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1 * step_count / EPS_DECAY)\n",
    "\n",
    "    # get a random number to compare to the treshold\n",
    "    # if the number is bigger - exploit the memory\n",
    "    # else - explore - get a random action\n",
    "    # the epsilon will reduce over time so the chance of sample being bigger than epsilon will increase\n",
    "    sample = random.random()\n",
    "    # increase the steps count\n",
    "    step_count += 1\n",
    "    \n",
    "    if sample > epsilon_treshold:\n",
    "        with torch.no_grad():\n",
    "            # get action from the memory\n",
    "            return policy_net(state).max(1)[1].view(1, 1) # this returns int, index of the action with maximum reward\n",
    "    else:        \n",
    "        return torch.tensor([[env.action_space.sample()]], dtype=torch.long)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dcdfda-8473-49e9-a015-a05964a29552",
   "metadata": {},
   "source": [
    "# create function to plot the episode duration to monitor learning progress\n",
    "episode_rewards = []\n",
    "\n",
    "def plot_rewards(show_result=False):\n",
    "\n",
    "    # convert the list to np array\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "    \n",
    "    plt.figure(1)\n",
    "    \n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training')\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(rewards_t.numpy(), label='Episode Rewards')\n",
    "\n",
    "    # Plot moving average of the last 100 episodes\n",
    "    if len(rewards_t) >= 100:\n",
    "        # Compute the moving average\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1)  # 100-episode window\n",
    "        means = torch.cat((torch.zeros(99), means))  # Pad with zeros for alignment\n",
    "        plt.plot(means.numpy(), label='100-Episode Reward Average')\n",
    "    \n",
    "    plt.pause(0.001)  # Pause to update the plot in real-time\n",
    "\n",
    "    # Display in Jupyter/IPython\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())   \n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c0b5e-b86c-4b0b-8286-8709a334d92e",
   "metadata": {},
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return  # Exit if there are not enough samples in memory to form a batch\n",
    "\n",
    "    # Sample a batch of transitions from the replay memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))  # Convert batch-array of transitions to Transition of batch-arrays\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool\n",
    "    )\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model's Q-values for the current states and actions\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Initialize tensor for the next state values\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    # Compute expected Q-values\n",
    "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
    "\n",
    "    # Compute Huber loss (Smooth L1 Loss)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()  # Clear the gradients from the previous step\n",
    "    loss.backward()  # Backpropagate the loss\n",
    "\n",
    "    # Clip gradients to avoid exploding gradients\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), clip_value=1.0)\n",
    "\n",
    "    # Perform a single optimization step\n",
    "    optimizer.step()\n",
    "    \n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41519e-c9cb-4200-947a-e3f651912e44",
   "metadata": {},
   "source": [
    "def soft_update_target_network(policy_net, target_net, tau=0.01):\n",
    "    \"\"\"\n",
    "    Perform a soft update of the target network parameters.\n",
    "\n",
    "    Args:\n",
    "        policy_net (nn.Module): The policy network whose parameters are used for the update.\n",
    "        target_net (nn.Module): The target network to be updated.\n",
    "        tau (float): The soft update coefficient. Values closer to 1.0 mean faster updates.\n",
    "    \"\"\"\n",
    "    for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "        target_param.data.copy_(tau * policy_param.data + (1.0 - tau) * target_param.data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063407d8-15f3-40b9-90d6-18727f7111c0",
   "metadata": {},
   "source": [
    "# training algorithm\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    rewards = 0\n",
    "\n",
    "    for t in count():\n",
    "        action = take_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            \n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # optimize the policy\n",
    "        optimize_model()\n",
    "\n",
    "        # soft update\n",
    "        soft_update_target_network(policy_net, target_net, tau=TAU)\n",
    "        \n",
    "        rewards += reward\n",
    "        if done:\n",
    "            episode_rewards.append(rewards)\n",
    "            plot_rewards()\n",
    "            break\n",
    "        \n",
    "        \n",
    "print('Completed')\n",
    "plot_rewards(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51fbcfe6-abf0-49cd-984c-75341b3527cf",
   "metadata": {},
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "# Reset the environment\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "for t in range(1000):  # Maximum steps for visualization\n",
    "    with torch.no_grad():\n",
    "        # Use the trained policy network to select the best action\n",
    "        action = policy_net(state).max(1)[1].view(1, 1).item()\n",
    "\n",
    "    # Take the action in the environment\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Render the environment\n",
    "    env.render()    \n",
    "\n",
    "    # Prepare the next state\n",
    "    if not terminated:\n",
    "        state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    else:\n",
    "        print(f\"Landed for {t+1} steps!\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e617f900-f46e-44cc-afaf-03baea96a8ee",
   "metadata": {},
   "source": [
    "# Initialise the environment in human mode\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\", continuous=False)\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    # this is where you would insert your policy\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d8290ca-ec60-4236-b9d2-b24b2a1b18d2",
   "metadata": {},
   "source": [
    "env.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6213673-eb87-481b-a019-15fc811c48e6",
   "metadata": {},
   "source": [
    "# Initialize the environment with the correct render mode\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Reset the environment\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Set up the plot\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(env.render())\n",
    "plt.axis('off')  # Hide axes for better visualization\n",
    "\n",
    "for _ in range(1000):\n",
    "    # Sample a random action\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Step through the environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Render the current frame\n",
    "    frame = env.render()\n",
    "\n",
    "    # Ensure the frame is a NumPy array\n",
    "    if isinstance(frame, np.ndarray):\n",
    "        img.set_data(frame)\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        print(\"Warning: env.render() did not return a valid frame.\")\n",
    "\n",
    "    # Check if the episode has ended\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a6200-e3f8-4ee0-8ee3-f06d84009f4b",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7445f-0bc3-4e78-a248-b9af831616dc",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
