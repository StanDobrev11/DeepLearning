{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89d4c2-d66b-4751-b084-6e70a615c895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this command in wsl:\n",
    "\n",
    "# xhost +local:root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8038aeec-ac9f-480b-9ce6-c00593eaf2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
      "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
      "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
      "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
      "Building wheels for collected packages: box2d-py\n",
      "  Building wheel for box2d-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2379448 sha256=061eb4c9d677dd5a3d0c27df0765fd8fd30d8991c6f0648d92935ac4eed223ab\n",
      "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
      "Successfully built box2d-py\n",
      "Installing collected packages: box2d-py\n",
      "Successfully installed box2d-py-2.3.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install swig\n",
    "!pip install gymnasium['box2d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4349eb56-f2fd-4fef-9b16-92ffbb1440f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50812c0f-3a50-4090-8f02-27ea9ee58a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96731c2e-7673-4679-b3a1-2589f1db37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "698d1eaa-70ee-4c26-b44c-151eb8d7717e",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "# enable interactive mode\n",
    "plt.ion()\n",
    "\n",
    "# setup the device to be used\n",
    "# all the tensors are to be sent to this device for rendering and manipulation\n",
    "# in this case video rendering\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff2589b-193d-4830-a786-8b31905095c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4da3559-b255-4ecd-8f3c-fbc04da24072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating subclass of tuple with named fields\n",
    "# this will be pushed to the replay memory\n",
    "Transition = namedtuple(typename='Transition', field_names=['state', 'action', 'next_state', 'reward'])\n",
    "\n",
    "# creating a class to store the memory to be replayed\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        param: capacity - maximum number of transition the buffer can hold\n",
    "        \"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # all the args ['state', 'action', 'next_state', 'reward'] must be pushed to the tensor's devide before stored in memory\n",
    "        state, action, next_state, reward = args\n",
    "        state = state.to(device)\n",
    "        action = action.to(device)\n",
    "        reward = reward.to(device)\n",
    "        if next_state is not None:\n",
    "            next_state = next_state.to(device)\n",
    "\n",
    "        self.memory.append(Transition(state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # sample the memory buffer\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # current size of the memory buffer\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cf746dc-886f-4f1a-91d4-83202153bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    # fully connected NN with 3 layers\n",
    "    # activation function 'RELU'\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4bc57af-9744-4406-8e3a-489e63da991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparams\n",
    "# the size of the batch for sampling from the memory at each training step\n",
    "# larger size improve stability but increase computational cost\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# starting value of the epsilon\n",
    "EPS_START = 0.9\n",
    "\n",
    "# final value of the epsilon\n",
    "EPS_END = 0.05\n",
    "\n",
    "# decay rate of the epsilon\n",
    "# larger value should stabilize the training\n",
    "EPS_DECAY = 1000\n",
    "\n",
    "# discount coef\n",
    "# values close to 1 favor long term rewards\n",
    "GAMMA = 0.99\n",
    "\n",
    "# determines how mutch target network is updated against policy network\n",
    "TAU = 0.005\n",
    "\n",
    "# learning rate of the agent\n",
    "LR = 1e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ba36b77-6f43-4dc7-b32e-e79fc5916758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "step_count = 1000\n",
    "EPS_END + (EPS_START - EPS_END) * math.exp(-1 * step_count / EPS_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91c1923a-0c23-441c-ae97-2fcbf40a9d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "\n",
    "n_observations = env.observation_space.shape[0]\n",
    "\n",
    "# initialize the policy and target networks\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "\n",
    "# copy the initial policy network weights to the target network\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# setup the optimizer to be used\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR)\n",
    "\n",
    "# setup the memory with certain amount of records (Transitions)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# to keep track of each episode steps for epsilon decay\n",
    "step_count = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdc54904-640d-4b80-ae4f-401417328bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(state):\n",
    "    # the function will return the action to be taken from memory buffer or random from the action space based on the epsilon treshold and the decay of epsilon\n",
    "    global step_count\n",
    "\n",
    "    epsilon_treshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1 * step_count / EPS_DECAY)\n",
    "\n",
    "    # get a random number to compare to the treshold\n",
    "    # if the number is bigger - exploit the memory\n",
    "    # else - explore - get a random action\n",
    "    # the epsilon will reduce over time so the chance of sample being bigger than epsilon will increase\n",
    "    sample = random.random()\n",
    "    # increase the steps count\n",
    "    step_count += 1\n",
    "    \n",
    "    if sample > epsilon_treshold:\n",
    "        with torch.no_grad():\n",
    "            # get action from the memory\n",
    "            return policy_net(state).max(1)[1].view(1, 1) # this returns int, index of the action with maximum reward\n",
    "    else:        \n",
    "        return torch.tensor([[env.action_space.sample()]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4dcdfda-8473-49e9-a015-a05964a29552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function to plot the episode duration to monitor learning progress\n",
    "episode_rewards = []\n",
    "\n",
    "def plot_rewards(show_result=False):\n",
    "\n",
    "    # convert the list to np array\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "    \n",
    "    plt.figure(1)\n",
    "    \n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training')\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(rewards_t.numpy(), label='Episode Rewards')\n",
    "\n",
    "    # Plot moving average of the last 100 episodes\n",
    "    if len(rewards_t) >= 100:\n",
    "        # Compute the moving average\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1)  # 100-episode window\n",
    "        means = torch.cat((torch.zeros(99), means))  # Pad with zeros for alignment\n",
    "        plt.plot(means.numpy(), label='100-Episode Reward Average')\n",
    "    \n",
    "    plt.pause(0.001)  # Pause to update the plot in real-time\n",
    "\n",
    "    # Display in Jupyter/IPython\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "631c0b5e-b86c-4b0b-8286-8709a334d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return  # Exit if there are not enough samples in memory to form a batch\n",
    "\n",
    "    # Sample a batch of transitions from the replay memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))  # Convert batch-array of transitions to Transition of batch-arrays\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool\n",
    "    )\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model's Q-values for the current states and actions\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Initialize tensor for the next state values\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    # Compute expected Q-values\n",
    "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
    "\n",
    "    # Compute Huber loss (Smooth L1 Loss)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()  # Clear the gradients from the previous step\n",
    "    loss.backward()  # Backpropagate the loss\n",
    "\n",
    "    # Clip gradients to avoid exploding gradients\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), clip_value=1.0)\n",
    "\n",
    "    # Perform a single optimization step\n",
    "    optimizer.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af41519e-c9cb-4200-947a-e3f651912e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update_target_network(policy_net, target_net, tau=0.01):\n",
    "    \"\"\"\n",
    "    Perform a soft update of the target network parameters.\n",
    "\n",
    "    Args:\n",
    "        policy_net (nn.Module): The policy network whose parameters are used for the update.\n",
    "        target_net (nn.Module): The target network to be updated.\n",
    "        tau (float): The soft update coefficient. Values closer to 1.0 mean faster updates.\n",
    "    \"\"\"\n",
    "    for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "        target_param.data.copy_(tau * policy_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "063407d8-15f3-40b9-90d6-18727f7111c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training algorithm\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    rewards = 0\n",
    "\n",
    "    for t in count():\n",
    "        action = take_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            \n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # optimize the policy\n",
    "        optimize_model()\n",
    "\n",
    "        # soft update\n",
    "        soft_update_target_network(policy_net, target_net, tau=TAU)\n",
    "        \n",
    "        rewards += reward\n",
    "        if done:\n",
    "            episode_rewards.append(rewards)\n",
    "            plot_rewards()\n",
    "            break\n",
    "        \n",
    "        \n",
    "print('Completed')\n",
    "plot_rewards(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51fbcfe6-abf0-49cd-984c-75341b3527cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "# Reset the environment\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "for t in range(1000):  # Maximum steps for visualization\n",
    "    with torch.no_grad():\n",
    "        # Use the trained policy network to select the best action\n",
    "        action = policy_net(state).max(1)[1].view(1, 1).item()\n",
    "\n",
    "    # Take the action in the environment\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Render the environment\n",
    "    env.render()    \n",
    "\n",
    "    # Prepare the next state\n",
    "    if not terminated:\n",
    "        state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    else:\n",
    "        print(f\"Landed for {t+1} steps!\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e617f900-f46e-44cc-afaf-03baea96a8ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# step (transition) through the environment with the action\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# receiving the next observation, reward and if the episode has terminated or truncated\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# If the episode has ended then we can reset to start a new episode\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/envs/box2d/lunar_lander.py:665\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    662\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 665\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# truncation=False as the time limit is handled by the `TimeLimit` wrapper added during `make`\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(state, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {}\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/gymnasium/envs/box2d/lunar_lander.py:779\u001b[0m, in \u001b[0;36mLunarLander.render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 779\u001b[0m     \u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(\n\u001b[1;32m    782\u001b[0m         np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    783\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialise the environment in human mode\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\", continuous=False)\n",
    "\n",
    "# Reset the environment to generate the first observation\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "    # this is where you would insert your policy\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # step (transition) through the environment with the action\n",
    "    # receiving the next observation, reward and if the episode has terminated or truncated\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # If the episode has ended then we can reset to start a new episode\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d8290ca-ec60-4236-b9d2-b24b2a1b18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6213673-eb87-481b-a019-15fc811c48e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment with the correct render mode\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Reset the environment\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# Set up the plot\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots()\n",
    "img = ax.imshow(env.render())\n",
    "plt.axis('off')  # Hide axes for better visualization\n",
    "\n",
    "for _ in range(1000):\n",
    "    # Sample a random action\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    # Step through the environment\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Render the current frame\n",
    "    frame = env.render()\n",
    "\n",
    "    # Ensure the frame is a NumPy array\n",
    "    if isinstance(frame, np.ndarray):\n",
    "        img.set_data(frame)\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "    else:\n",
    "        print(\"Warning: env.render() did not return a valid frame.\")\n",
    "\n",
    "    # Check if the episode has ended\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a6200-e3f8-4ee0-8ee3-f06d84009f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7445f-0bc3-4e78-a248-b9af831616dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
