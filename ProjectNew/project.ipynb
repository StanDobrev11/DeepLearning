{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b02451-c710-4547-8cac-8f795470bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade grpcio grpcio-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8799ec51-d0be-4c4f-a4ad-4e8ad539f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "605a33c7-17ed-42c1-9e6e-be50b0addb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 21:02:35.329499: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-28 21:02:35.339931: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738098155.352847     780 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738098155.356702     780 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-28 21:02:35.372030: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.spaces.utils import flatten\n",
    "from gymnasium.envs.registration import register, registry\n",
    "import time\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Any, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorboard\n",
    "\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import VecNormalize, DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "import environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6482d68a-fce7-40e6-8d5a-275862ba21e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'MarineEnv-v0' not in registry:\n",
    "    register(\n",
    "        id='MarineEnv-v0',\n",
    "        entry_point='environments:MarineEnv',  # String reference to the class\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1337d-61df-4e2c-ae20-98ec70f98b2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1bef0-7e94-4b1f-abe8-6e917fb7a917",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "def make_env():\n",
    "    env = gym.make('MarineEnv-v0', render_mode='rgb_array', continuous=True, max_episode_steps=400)\n",
    "    env = Monitor(env)  # ✅ Apply Monitor FIRST before vectorization\n",
    "    return env\n",
    "\n",
    "# Wrap it in `DummyVecEnv` FIRST\n",
    "env = DummyVecEnv([make_env])  \n",
    "\n",
    "# Now apply normalization\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c63fc0-09d2-4f9c-98a3-6f4539521a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_kwargs = dict(\n",
    "    render_mode='rgb_array',\n",
    "    continuous=True,\n",
    "    max_episode_steps=400,\n",
    "    training_stage=2,\n",
    "    timescale=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceee0548-ec75-47e1-afc1-09c26ce4ffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = 8  # Number of parallel environments\n",
    "vec_env = make_vec_env(env_id='MarineEnv-v0', n_envs=n_envs, env_kwargs=env_kwargs)\n",
    "env = gym.make('MarineEnv-v0', **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3375503c-fcdc-4603-9365-e07a4e7bf91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = { \n",
    "    'clip_range': 0.2,  # Reduce to prevent large updates\n",
    "    'ent_coef': 0.01,  # Higher entropy to encourage exploration\n",
    "    'gamma': 0.99, \n",
    "    'learning_rate': 3e-4,  # Slightly higher for better learning\n",
    "    'n_steps': 2048,  # Increase from default (512) to 2048\n",
    "    'batch_size': 512,  # Adjust batch size for stability\n",
    "    'gae_lambda': 0.95,  # Generalized Advantage Estimation smoothing\n",
    "    'max_grad_norm': 0.9, \n",
    "    'policy_kwargs': {'net_arch': [256, 256], 'activation_fn': torch.nn.Tanh},  # Slightly deeper network\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6413b289-fc9b-4df1-a806-7d1dc1a33333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "    policy='MlpPolicy',\n",
    "    env=env,\n",
    "    # env=vec_env,\n",
    "    verbose=1,\n",
    "    device='cpu', \n",
    "    tensorboard_log='./stage_1_tensorboard_logs/',\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947167ce-ade1-4036-a3bf-356ae7b9f00a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./stage_1_tensorboard_logs/ppo_0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90e49f635a14e9eaf82fbad547ba86c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.1     |\n",
      "|    ep_rew_mean     | 346      |\n",
      "| time/              |          |\n",
      "|    fps             | 40       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 83.8         |\n",
      "|    ep_rew_mean          | 371          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 40           |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 101          |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054543554 |\n",
      "|    clip_fraction        | 0.0592       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.00253      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+03     |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0102      |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 5.58e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 80.3         |\n",
      "|    ep_rew_mean          | 417          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 39           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 155          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034223273 |\n",
      "|    clip_fraction        | 0.0252       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.00597      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.6e+03      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00673     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 5.6e+03      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 80           |\n",
      "|    ep_rew_mean          | 358          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 39           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 205          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031245411 |\n",
      "|    clip_fraction        | 0.0138       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.0695       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.16e+03     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00808     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.45e+03     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 80.4         |\n",
      "|    ep_rew_mean          | 379          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 39           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 256          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039438996 |\n",
      "|    clip_fraction        | 0.0229       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.0492       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.62e+03     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.008       |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.56e+03     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 80.3        |\n",
      "|    ep_rew_mean          | 378         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 39          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 311         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004377089 |\n",
      "|    clip_fraction        | 0.0499      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.0154      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.08e+03    |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00907    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.43e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 74.2         |\n",
      "|    ep_rew_mean          | 406          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 38           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 370          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056796772 |\n",
      "|    clip_fraction        | 0.0347       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.00977      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.05e+03     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00895     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 6.15e+03     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=(3e4), reset_num_timesteps=False, progress_bar=True, tb_log_name='ppo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "466d533e-0a5d-4c2d-aaf5-467bad256d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 129.08, Std: 243.09\n"
     ]
    }
   ],
   "source": [
    "eval_env = gym.make('MarineEnv-v0', **env_kwargs)\n",
    "mean, std = evaluate_policy(model=model, env=eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f'Mean: {mean:.2f}, Std: {std:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36047618-2527-4c6b-b083-78c697c3e984",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill 5813"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22985742-c740-42fa-8382-f042d32ddb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d39ac50237491f8f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d39ac50237491f8f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./stage_1_tensorboard_logs/ --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce986e-7b91-4254-a25c-42bc26d7954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save environment normalization stats\n",
    "# env.save(\"ppo_normalized_env.pkl\")\n",
    "# model.save(\"ppo_marine_stage_2\")\n",
    "model = model.load(\"ppo_marine_stage_2\")\n",
    "# model = model.load('ppo_marine_stage_1.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e5042a-6ba5-4092-b202-5b02a635b376",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "env = VecNormalize.load(\"ppo_normalized_env.pkl\", env)\n",
    "\n",
    "# Disable reward normalization for evaluation\n",
    "env.training = False\n",
    "env.norm_reward = False\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, dones, _ = env.step(action)\n",
    "\n",
    "    # ✅ Ensure env.get_images() is not empty\n",
    "    images = env.get_images()\n",
    "    if images and images[0] is not None:\n",
    "        frame = images[0]\n",
    "        \n",
    "        # ✅ Ensure the frame has valid dimensions before displaying\n",
    "        if frame.shape[0] > 0 and frame.shape[1] > 0:\n",
    "            cv2.imshow(\"PPO MarineEnv Evaluation\", frame)\n",
    "            cv2.waitKey(1)  # Display for 1ms\n",
    "        else:\n",
    "            print(\"Warning: Received an empty frame from env.get_images()\")\n",
    "\n",
    "    if dones:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "cv2.destroyAllWindows()  # Close display window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec1768e6-1c27-4e5b-9ffc-66d9743dad97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990.6062411069894\n",
      "990.6062411069894\n",
      "[92.87706    13.425813    0.20684034  0.9243701  -3.4524682  -1.0853267\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ]\n",
      "-439.70000000000005\n",
      "-439.70000000000005\n",
      "[ 2.6545068e+02  1.3592867e+01  2.3217756e-01  1.0248504e+00\n",
      " -2.5107893e+01 -3.5882111e+00  5.0154132e-01  4.5699403e-01\n",
      "  3.0039322e+02  4.7431034e-01  4.3743954e+01  5.4724388e+01\n",
      "  8.5442839e+00  7.6221085e+00  0.0000000e+00  8.9176244e-01\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "1125.0399366378783\n",
      "1125.0399366378783\n",
      "[ 23.376272    12.099783     0.22011055   1.0914769   -0.69244534\n",
      " -11.368698     0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.        ]\n",
      "-62.323998230695224\n",
      "-62.323998230695224\n",
      "[120.98082     13.912072     0.23162441   0.99894994  -3.7248056\n",
      "  -3.6004791    0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.        ]\n",
      "-660.5598242163655\n",
      "-660.5598242163655\n",
      "[ 2.6380954e+02  6.9683161e+00  2.1776137e-01  1.8750129e+00\n",
      " -5.0616417e+00 -3.2308289e+01  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  0.0000000e+00  0.0000000e+00  0.0000000e+00]\n",
      "1005.8287685275077\n",
      "1005.8287685275077\n",
      "[ 10.251818     8.395625     0.20177963   1.4420341   -1.2228019\n",
      " -18.162052     0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.        ]\n",
      "1241.1175264358521\n",
      "1241.1175264358521\n",
      "[16.966747  10.714543   0.2086887  1.1686287 -0.7507377 -9.488014\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.       ]\n",
      "-232.397937136888\n",
      "-232.397937136888\n",
      "[154.97806     12.249333     0.22087236   1.0818826   -6.0920563\n",
      "  -2.6054683    0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.        ]\n",
      "700.4830361306667\n",
      "700.4830361306667\n",
      "[ 66.22171     16.848423     0.20055856   0.71422195   0.2376991\n",
      " -10.086641     0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-828.0148641943933\n",
      "-828.0148641943933\n",
      "[  7.0436273    7.927597     0.21248421   1.6081862   -0.58792377\n",
      " -13.476803     0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.           0.           0.           0.           0.\n",
      "   0.        ]\n"
     ]
    }
   ],
   "source": [
    "timescale = 1 / 6\n",
    "for _ in range(10):\n",
    "    env = gym.make('MarineEnv-v0', render_mode='human', continuous=True, training_stage=2, timescale=timescale)\n",
    "    state, _ = env.reset()\n",
    "    # print(state)\n",
    "    episode_rewards = 0 \n",
    "    # flatten_state = flatten(env.observation_space, state)\n",
    "    # state = torch.tensor(flatten_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for _ in range(int(400 / timescale)):\n",
    "        action = model.predict(state, deterministic=True)\n",
    "        # print(action)\n",
    "        # observation, reward, terminated, truncated, info = env.step((0, 0))\n",
    "        observation, reward, terminated, truncated, info = env.step(action[0])\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "        episode_rewards += reward\n",
    "        # print('===========================')\n",
    "        # print(observation)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            print(episode_rewards)\n",
    "            break\n",
    "    \n",
    "        state = observation\n",
    "            \n",
    "    print(episode_rewards)\n",
    "    print(state)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d50ff7-6e91-4359-a0a3-7bb1259dfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_kwargs['render_mode'] = 'human'\n",
    "\n",
    "# vec_env = make_vec_env(env_id='MarineEnv-v0', n_envs=n_envs, env_kwargs=env_kwargs)\n",
    "# vec_env.unwrapped.timescale = 1 / 3\n",
    "\n",
    "# state = vec_env.reset()\n",
    "# # print(state)\n",
    "# episode_rewards = 0 \n",
    "# # flatten_state = flatten(env.observation_space, state)\n",
    "# # state = torch.tensor(flatten_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "# for _ in range(400):\n",
    "#     action = model.predict(state, deterministic=True)\n",
    "#     # print(action)\n",
    "#     # observation, reward, terminated, truncated, info = env.step((0, 0))\n",
    "#     observation, reward, dones, info = vec_env.step(action[0])\n",
    "#     vec_env.render()\n",
    "#     time.sleep(0.05)\n",
    "#     episode_rewards += reward\n",
    "#     # print('===========================')\n",
    "#     # print(observation)\n",
    "#     # if terminated or truncated:\n",
    "#     #     print(episode_rewards)\n",
    "#     #     break\n",
    "\n",
    "#     state = observation\n",
    "        \n",
    "# print(episode_rewards)\n",
    "# print(state)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde91879-83a6-40a1-ac60-f39cf4dba50d",
   "metadata": {},
   "source": [
    "# Optimizing hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc360def-2b77-408a-b156-ad13a88c6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS = 100  # Maximum number of trials\n",
    "N_JOBS = 1 # Number of jobs to run in parallel\n",
    "N_STARTUP_TRIALS = 5  # Stop random sampling after N_STARTUP_TRIALS\n",
    "N_EVALUATIONS = 2  # Number of evaluations during the training\n",
    "N_TIMESTEPS = int(2e4)  # Training budget\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_ENVS = 10\n",
    "N_EVAL_EPISODES = 10\n",
    "TIMEOUT = int(60 * 15)  # 15 minutes\n",
    "\n",
    "ENV_ID = 'MarineEnv-v0'\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": ENV_ID,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e27a9-c6ec-4c1b-a68c-01c2a7eaf53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ppo_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1, log=True)  # Learning rate (log scale)\n",
    "    \n",
    "    n_steps = 2 ** trial.suggest_int('n_steps', 7, 12)  # Number of steps per update (512-4096)\n",
    "    \n",
    "    batch_size = 2 ** trial.suggest_int('batch_size', 5, 10)  # Minibatch size (32-1024)\n",
    "    \n",
    "    gamma = trial.suggest_float('gamma', 0.9, 0.9999)  # Discount factor (close to 1 for long-term rewards)\n",
    "    \n",
    "    gae_lambda = trial.suggest_float('gae_lambda', 0.8, 1.0)  # GAE lambda (trade-off bias/variance)\n",
    "    \n",
    "    clip_range = trial.suggest_float('clip_range', 0.1, 0.3)  # PPO clipping range\n",
    "    \n",
    "    ent_coef = trial.suggest_float('ent_coef', 0.0001, 0.1, log=True)  # Entropy coefficient (for exploration)\n",
    "    \n",
    "    vf_coef = trial.suggest_float('vf_coef', 0.1, 1.0)  # Value function loss coefficient\n",
    "    \n",
    "    max_grad_norm = trial.suggest_float('max_grad_norm', 0.3, 5.0)  # Gradient clipping\n",
    "    \n",
    "    target_kl = trial.suggest_float('target_kl', 0.01, 0.2)  # KL divergence target\n",
    "    \n",
    "    n_epochs = trial.suggest_int('n_epochs', 3, 10)  # PPO update epochs per batch\n",
    "    \n",
    "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu'])\n",
    "    \n",
    "    net_arch = trial.suggest_categorical('net_arch', ['tiny', 'small'])\n",
    "    \n",
    "    # Convert architecture choices\n",
    "    net_arch = [128, 128] if net_arch == 'tiny' else [256, 256]\n",
    "    \n",
    "    activation_fn = {'tanh': nn.Tanh, 'relu': nn.ReLU}[activation_fn]\n",
    "    \n",
    "    # Store gamma value in Optuna logs\n",
    "    trial.set_user_attr('gamma', gamma)\n",
    "\n",
    "    return {\n",
    "        'n_steps': n_steps,\n",
    "        'batch_size': batch_size,\n",
    "        'gamma': gamma,\n",
    "        'gae_lambda': gae_lambda,\n",
    "        'learning_rate': learning_rate,\n",
    "        'clip_range': clip_range,\n",
    "        'ent_coef': ent_coef,\n",
    "        'vf_coef': vf_coef,\n",
    "        'max_grad_norm': max_grad_norm,\n",
    "        'target_kl': target_kl,\n",
    "        'n_epochs': n_epochs,\n",
    "        'policy_kwargs': {\n",
    "            'net_arch': net_arch,\n",
    "            'activation_fn': activation_fn\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b399f-2be5-4a8d-9e5f-2aa974ab5a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    Callback used for evaluating and reporting a trial.\n",
    "    \n",
    "    :param eval_env: Evaluation environement\n",
    "    :param trial: Optuna trial object\n",
    "    :param n_eval_episodes: Number of evaluation episodes\n",
    "    :param eval_freq:   Evaluate the agent every ``eval_freq`` call of the callback.\n",
    "    :param deterministic: Whether the evaluation should\n",
    "        use a stochastic or deterministic policy.\n",
    "    :param verbose:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Evaluate policy (done in the parent class)\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            # Send report to Optuna\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1217b67-1e97-4470-b153-8732753c9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function using by Optuna to evaluate\n",
    "    one configuration (i.e., one set of hyperparameters).\n",
    "\n",
    "    Given a trial object, it will sample hyperparameters,\n",
    "    evaluate it and report the result (mean episodic reward after training)\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: Mean episodic reward after training\n",
    "    \"\"\"\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "\n",
    "    # 1. Sample hyperparameters and update the keyword arguments\n",
    "    kwargs.update(**sample_ppo_params(trial))\n",
    "    print(kwargs)\n",
    "    # Create the RL model\n",
    "    model = PPO(device='cpu', verbose=1, **kwargs)\n",
    "    # Create eval envs\n",
    "    eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS)\n",
    "\n",
    "    eval_callback = TrialEvalCallback(eval_envs, trial, N_EVAL_EPISODES, EVAL_FREQ, deterministic=True, verbose=0)\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        # Train the model\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback, progress_bar=True)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_envs.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54709eec-77e8-47fb-b3ff-f1d8c6848ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pytorch num threads to 1 for faster training\n",
    "torch.set_num_threads(1)\n",
    "# Select the sampler, can be random, TPESampler, CMAES, ...\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(\n",
    "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3\n",
    ")\n",
    "# Create the study and start the hyperparameter optimization\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value: {trial.value}\")\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Write report\n",
    "study.trials_dataframe().to_csv(\"study_results_ppo_marineenv.csv\")\n",
    "\n",
    "fig1 = plot_optimization_history(study)\n",
    "fig2 = plot_param_importances(study)\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
