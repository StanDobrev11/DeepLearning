{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414a8cd2-090d-423d-939f-2cf8bf3f0a91",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5efcfc-77cd-4caf-bba2-ad195836fb14",
   "metadata": {},
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from gymnasium.envs.registration import register, registry\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3528720-6b5d-434b-a308-8c9a0e3efa69",
   "metadata": {},
   "source": [
    "if 'MarineEnv-v0' not in registry:\n",
    "    register(\n",
    "        id='MarineEnv-v0',\n",
    "        entry_point='environments:MarineEnv',  # String reference to the class\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7b975cd-0a3a-4ff1-b85b-63e355641af8",
   "metadata": {},
   "source": [
    "env_kwargs = dict(\n",
    "    render_mode='rgb_array',\n",
    "    continuous=True,\n",
    "    max_episode_steps=1200,\n",
    "    training_stage=2,\n",
    "    timescale=1/3\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c0eea1c-fd66-4e71-81af-31cefd581ff1",
   "metadata": {},
   "source": [
    "env = make_vec_env(env_id=\"MarineEnv-v0\", n_envs=1, env_kwargs=env_kwargs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa5b30b-6633-44b2-9a76-7a52f931b0ca",
   "metadata": {},
   "source": [
    "td3_kwargs = {\n",
    "    \"policy\": \"MlpPolicy\",  # Multi-Layer Perceptron policy\n",
    "    \"learning_rate\": 3e-4,  # Stable learning rate for TD3\n",
    "    \"buffer_size\": int(1e6),  # Large replay buffer for off-policy learning\n",
    "    \"learning_starts\": 10000,  # Start training after collecting enough samples\n",
    "    \"batch_size\": 256,  # Larger batch size stabilizes updates\n",
    "    \"tau\": 0.005,  # Polyak averaging coefficient for target networks\n",
    "    \"gamma\": 0.99,  # Discount factor (high for long-term planning)\n",
    "    \"train_freq\": (1, \"step\"),  # Train every step\n",
    "    \"gradient_steps\": 1,  # One gradient update per environment step\n",
    "    \"action_noise\": None,  # TD3 handles exploration differently\n",
    "    \"replay_buffer_class\": None,  # Use default replay buffer\n",
    "    \"optimize_memory_usage\": False,  # Avoid memory-efficient mode for stability\n",
    "    \"policy_delay\": 2,  # Update actor less frequently than critics (TD3 trick)\n",
    "    \"target_policy_noise\": 0.2,  # Target policy smoothing noise (TD3 trick)\n",
    "    \"target_noise_clip\": 0.5,  # Clip noise to avoid instability\n",
    "    \"tensorboard_log\": \"./tensorboard_td3_asv/\",  # TensorBoard logging path\n",
    "    \"policy_kwargs\": {\n",
    "        \"net_arch\": [256, 256],  # Deep enough for ASV navigation\n",
    "        \"activation_fn\": torch.nn.ReLU,  # ReLU activation for stability\n",
    "    },\n",
    "    \"verbose\": 1,  # Print training updates\n",
    "    \"device\": \"auto\",  # Use GPU if available\n",
    "}\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9765ed00-bfaf-477e-a4ce-ef4689cefd76",
   "metadata": {},
   "source": [
    "agent = TD3(env=env, **td3_kwargs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d161ec1-9c2a-44e4-ab26-571728599bae",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "agent.learn(total_timesteps=1e5, reset_num_timesteps=False, progress_bar=True, tb_log_name='td3_2')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a69d495-2662-4c72-a1b0-a87276b543f6",
   "metadata": {},
   "source": [
    "eval_env = gym.make('MarineEnv-v0', **env_kwargs)\n",
    "mean, std = evaluate_policy(model=agent, env=eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f'Mean: {mean:.2f}, Std: {std:.2f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a065463e-bc6b-47be-a435-67706bca846f",
   "metadata": {},
   "source": [
    "agent.save('td3_asv')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9df15eeb-7c92-4bd1-8d54-3303f29e7cb1",
   "metadata": {},
   "source": [
    "%tensorboard --logdir ./tensorboard_td3_asv/ --host=0.0.0.0"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4015d194-271f-4c88-96b3-9e692f8f9e2f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "timescale = 1 / 6\n",
    "for _ in range(5):\n",
    "    env = gym.make('MarineEnv-v0', render_mode='human', continuous=True, training_stage=2, timescale=timescale, training=False)\n",
    "    state, _ = env.reset()\n",
    "    print(state)\n",
    "    episode_rewards = 0 \n",
    "    # flatten_state = flatten(env.observation_space, state)\n",
    "    # state = torch.tensor(flatten_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for _ in range(int(400 / timescale)):\n",
    "        action = agent.predict(state, deterministic=True)\n",
    "        # print(action)\n",
    "        # observation, reward, terminated, truncated, info = env.step((0, 0))\n",
    "        observation, reward, terminated, truncated, info = env.step(action[0])\n",
    "        env.render()\n",
    "        # time.sleep(0.001)\n",
    "        episode_rewards += reward\n",
    "        print('===========================')\n",
    "        print(observation)\n",
    "        print(reward)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            print(episode_rewards)\n",
    "            break\n",
    "    \n",
    "        state = observation\n",
    "            \n",
    "    print(episode_rewards)\n",
    "    print(state)\n",
    "    env.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487de602-36a9-4d75-b94d-fc2aea58aaf2",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
