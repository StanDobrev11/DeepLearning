{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d693dff-3da0-4c98-83e3-b71ddf06910a",
   "metadata": {},
   "source": [
    "# Summary on the \"[Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347v2)\" article by John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fca790-ba2e-4811-afbb-050525ba6e7b",
   "metadata": {},
   "source": [
    "## Introduction to the article\n",
    "\n",
    "In this article, the authors introduce a new group of reinforcement learning (RL) methods called Proximal Policy Optimization (PPO). \n",
    "The key idea behind PPO is to improve how RL agents learn from their environment. It works by:\n",
    "\n",
    "- Interacting with the Environment: The agent collects data by taking actions and observing the results, just like in traditional RL methods.\n",
    "- Optimizing with Multiple Updates: Unlike standard methods that update the agent’s policy after each new data sample, PPO allows for multiple updates using the same data, making learning more efficient.\n",
    "\n",
    "PPO is inspired by another algorithm called Trust Region Policy Optimization (TRPO). While TRPO is effective, it’s complex to implement. PPO offers similar benefits but is simpler, more flexible, and uses data more efficiently.\n",
    "\n",
    "**Goals of the Article**\n",
    "1. Introduce PPO: Explain how PPO works and what makes it different from other RL algorithms.\n",
    "2. Demonstrate PPO’s Efficiency: Show that PPO can learn faster and perform better with less data (better sample efficiency).\n",
    "3. Test Across Tasks: Evaluate PPO on different benchmark environments, such as robotic simulations and Atari games, to prove its versatility.\n",
    "4. Compare with Other Methods: Show that PPO outperforms other common RL algorithms in terms of performance, simplicity, and training time.\n",
    "\n",
    "In short, the article aims to present PPO as an effective, easy-to-use algorithm that balances strong performance with efficient learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e11aa-5a4d-4edc-bcbd-64c78ee4e7d6",
   "metadata": {},
   "source": [
    "## What is PPO and how it works\n",
    "\n",
    "PPO (Proximal Policy Optimization) is based on policy gradient methods, which are a way for an **agent** (like a robot, autonomous vehicle, or game-playing AI) to learn how to make better decisions over time. The goal of the method is to improve the strategy of the agent, with the strategy being the way the agent chooses an action. This strategy is called the policy.\n",
    "\n",
    "By interacting with the environment, the agent obtains the results of its actions, called rewards, along with information on how the environment changes. After gathering the data—state, action, reward, and next state—the policy is then updated using gradient ascent methods. The formula used is:\n",
    "$$\\hat{g} = \\hat{E}_t \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) \\hat{A}_t \\right]$$\n",
    "\n",
    "where $\\pi_{\\theta}(a_t | s_t)$ is the **policy** - telling the agent the probability of taking an action $a_t$ when it is in state $s_t$, \n",
    "\n",
    "$\\hat{A}_t$ is the advantage function, which compares how good an action is compared to the average action. If an action leads to a better outcome than expected, its advantage is high,\n",
    "\n",
    "$\\nabla_{\\theta}$ represents the gradient, calculating how much to change the policy to improve performance,\n",
    "\n",
    "$\\hat{E}_t$ is the empirical expectation, meaning the average outcome or expectation based on the data gathered.\n",
    "\n",
    "The loss of the objective function is calculated by differentiating the objective. However, performing multiple updates of the policy based on the loss from a single step is not recommended because it can lead to large policy updates, which can destabilize the training process.\n",
    "\n",
    "The article then explains the TRPO whis is a reinforcement learning algorithm designed to improve an agent’s policy while keeping updates stable and safe. The problem with regular policy gradient methods is that large updates to the policy can make learning unstable. TRPO solves this by restricting how much the policy can change in one update. It tries to maximize a special function called the **surrogate objective**, which estimates how good a new policy is compared to the old one. The goal is to improve the policy without making huge changes. This is done using the formula:\n",
    "$$\\begin{align}\n",
    "\\max_{\\theta} \\; & \\hat{E}_t \\left[ \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t \\right] \\tag{3} \\\\\n",
    "\\text{subject to} \\; & \\hat{E}_t \\left[ KL\\left( \\pi_{\\theta_{\\text{old}}}(\\cdot | s_t), \\pi_{\\theta}(\\cdot | s_t) \\right) \\right] \\leq \\delta \\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\delta$ is small value that controls how much the policy is allowed to change. This is the **constrain**. It limit how much the new policy can differ from the old policy. This ensures that updates are small and stable, and\n",
    "\n",
    "KL: The Kullback-Leibler (KL) divergence, which measures the difference between the old and new policies, measuring the probability distributions for all actions rather than only one. The other option is to use **penalty**:\n",
    "$$\n",
    "\\max_{\\theta} \\; \\hat{E}_t \\left[ \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t - \\beta \\, KL\\left( \\pi_{\\theta_{\\text{old}}}(\\cdot | s_t), \\pi_{\\theta}(\\cdot | s_t) \\right) \\right]\n",
    "$$\n",
    "\n",
    "where $\\beta$ is penalty "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1066f96-f1ce-435c-8c16-cc6ab6048216",
   "metadata": {},
   "source": [
    "Since no official code is submitted in support of the paper, below is my implementation of PPO algorithm employed on the continuous environment of MountainCarContinuous-v0. The environment is chosen as it is considered one of the simplest to begin and employ PPO algorithm in a continuous environment. The link to the descriotpion and goals of the environment can be found [here](https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3ceefc7-6f85-4146-82c1-dd735adcb322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-06 08:23:05.980820: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738830186.006518 2413834 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738830186.015325 2413834 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-06 08:23:06.041608: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import RecordEpisodeStatistics\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "998096d7-e73b-45ad-8b7d-dc61325bdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acccfd2-c16f-490f-aebf-10b61889bc90",
   "metadata": {},
   "source": [
    "The observation is a ndarray with shape (2,) where the elements correspond to the position of the car along the x-axis and the velocity of the car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45adfe51-5848-4dc5-a87b-bde16c5e8f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f88060-b802-4de8-8e9f-528a3766ff32",
   "metadata": {},
   "source": [
    "The action is a ndarray with shape (1,), representing the directional force applied on the car. The action is clipped in the range [-1,1] and multiplied by a power of 0.0015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c23a3360-3f73-4a49-9d67-30d92c5f6c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(-1.0, 1.0, (1,), float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d88749-53a2-4191-b1ad-6fbb5b68a92e",
   "metadata": {},
   "source": [
    "First, lets establish the policy or the neural network. It will take as input the observation space dimension and as output - mu (mean) and sigma (standart) of Normal Gaussian distribution. Then will be defining the layers. Will be one hidden layer of size 64 with ReLU activation. The value function will be handled by the 'critic' part of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89d1ce59-3375-4168-99dc-83dcb79ffb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Detect GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_space_dim: int, action_space_dim: int):\n",
    "        super().__init__()\n",
    "        self.obs_space_dim = obs_space_dim\n",
    "        self.action_space_dim = action_space_dim\n",
    "\n",
    "        self.input_layer = nn.Linear(obs_space_dim, 64)\n",
    "        self.hidden_layer = nn.Linear(64, 64)\n",
    "\n",
    "        # Actor, providing the actions\n",
    "        self.mu_layer = nn.Linear(64, action_space_dim)\n",
    "        self.sigma_layer = nn.Linear(64, action_space_dim)\n",
    "\n",
    "        # Critic, providing the value for that actions\n",
    "        self.value_layer = nn.Linear(64, 1)  # outputs one scalar value\n",
    "\n",
    "        # to device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = F.relu(self.hidden_layer(x))\n",
    "\n",
    "        # activate the output layers for the Actor\n",
    "        mu = torch.tanh(self.mu_layer(x)) # activated and bounded in range [-1, 1] in order to sample the action according to the observation space limits\n",
    "        # sigma = torch.clamp(F.softplus(self.sigma_layer(x)), 1e-03, 1.0) # this will ensure always positive sigma and clamped till 1.0\n",
    "        sigma = F.softplus(self.sigma_layer(x)) + 1e-05\n",
    "        # no activation applied for the Critic\n",
    "        value = self.value_layer(x)\n",
    "\n",
    "        return mu, sigma, value    \n",
    "\n",
    "    def act(self, state):\n",
    "        # check if state is tensor, if not convert\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        # predict mu, sigma and critic value of the current policy\n",
    "        mu, sigma, value = self.forward(state)\n",
    "\n",
    "        # generate normal distribuiton with mu and sigma\n",
    "        dist = Normal(mu, sigma)\n",
    "\n",
    "        # sample the distribution and get the log_probability\n",
    "        action = torch.tanh(dist.sample())\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action, log_prob, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85c8d56-3488-4e31-b71b-ce85e4adbf75",
   "metadata": {},
   "source": [
    "The trajectory data must be stored. Trajectory data is the data, generated each time the agent takes an action into the environment. There are various ways of doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "395df3eb-9fd0-4bd0-8a25-771865f36a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states: list = []\n",
    "        self.actions: list = []\n",
    "        self.log_probs: list = []\n",
    "        self.rewards: list = []\n",
    "        self.dones: list = []\n",
    "        self.critic_values: list = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.log_probs.clear()\n",
    "        self.rewards.clear()\n",
    "        self.dones.clear()\n",
    "        self.critic_values.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "669cbcf3-78c4-47af-8a5f-acac0a79640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae(next_value: list[int], rewards, dones: list[tuple[bool, bool]], values: list[int], gamma: float, lam: float) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute Generalized Advantage Estimation (GAE).\n",
    "\n",
    "    GAE helps to reduce variance in policy gradient methods while maintaining low bias. \n",
    "    It computes the advantage estimates and the corresponding discounted returns for each timestep.\n",
    "\n",
    "    Args:\n",
    "        next_value (list[int]): Value of the next state after the final step (scalar or list of scalars).\n",
    "        rewards (list[float]): List of rewards collected during the episode.\n",
    "        dones (list[tuple[bool, bool]]): List of done flags (terminated, truncated) indicating episode termination.\n",
    "        values (list[int]): List of value predictions from the value function (for each state).\n",
    "        gamma (float): Discount factor for future rewards (typically between 0.9 and 0.99).\n",
    "        lam (float): GAE lambda parameter controlling the bias-variance trade-off (typically between 0.9 and 0.95).\n",
    "\n",
    "    Returns:\n",
    "        tuple[torch.Tensor, torch.Tensor]: \n",
    "            - **returns (torch.Tensor):** Discounted cumulative rewards (returns) for each timestep.\n",
    "            - **advantages (torch.Tensor):** Advantage estimates normalized for stable learning.\n",
    "    \"\"\"\n",
    "    values = values + [next_value]\n",
    "    gae = 0  # Initial value of the advantage\n",
    "    returns = deque([])\n",
    "    advantages = deque([])\n",
    "\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        delta = rewards[step] + gamma * values[step + 1] * (1 - dones[step]) - values[step]\n",
    "        gae = delta + gamma * lam * (1 - dones[step]) * gae\n",
    "        returns.appendleft(gae + values[step])\n",
    "        advantages.appendleft(gae)\n",
    "\n",
    "    # Convert returns and advantages to tensors\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    advantages = torch.tensor(advantages, dtype=torch.float32)\n",
    "\n",
    "    # Normalize advantages\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    return returns, advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256b05a6-6d07-4010-8ef5-b0fba0b1c07c",
   "metadata": {},
   "source": [
    "The model and the optimizer will be initialized using defaut parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89bf5ddb-9c4c-4205-a683-010635111773",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_size = env.observation_space.shape[0]\n",
    "act_size = env.action_space.shape[0]\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.85\n",
    "EPOCHS = 4\n",
    "CLIP_EPS = 0.3\n",
    "LEARNING_RATE = 6e-4\n",
    "\n",
    "# initializing the agent and the optimizer.\n",
    "agent = ActorCritic(obs_size, act_size).to(device)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8db860-e3d0-4211-a66d-78373590f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, agent: ActorCritic, optimizer: optim.Adam, num_steps: int, writer, episode_count, printit=False) -> ActorCritic:   \n",
    "    memory = Memory()\n",
    "    state, _ = env.reset()\n",
    "    memory.clear()\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_length = 0\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # get the action, log_prob and value\n",
    "        action, log_prob, value = agent.act(state)\n",
    "\n",
    "        if printit:\n",
    "            print(f'Action: {action}\\nLog Prob: {log_prob}\\nValue: {value}')\n",
    "\n",
    "        # take a step into the environment\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        # set the termination flag\n",
    "        done = terminated or truncated\n",
    "\n",
    "        # add all to the memory\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.log_probs.append(log_prob)\n",
    "        memory.rewards.append(reward)\n",
    "        memory.dones.append(done)\n",
    "        memory.critic_values.append(value)\n",
    "\n",
    "        # track episode stats\n",
    "        episode_reward += reward\n",
    "        episode_length += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # when the episoed is completed, calculate the GAE\n",
    "    if done:\n",
    "        next_value = 0\n",
    "    else:\n",
    "        _, _, next_value = agent.act(state)\n",
    "    \n",
    "    returns, advantages = compute_gae(\n",
    "        next_value=next_value,\n",
    "        rewards=memory.rewards,\n",
    "        dones=memory.dones,\n",
    "        values=memory.critic_values,\n",
    "        gamma=GAMMA,\n",
    "        lam=LAMBDA,\n",
    "    ) \n",
    "\n",
    "    # updating the policy of the agent.\n",
    "    # iterate over the collected trajectories for the set epochs\n",
    "    for _ in range(EPOCHS):\n",
    "        # set the gradients of the optimizer to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # convert the states to tensor for faster computing\n",
    "        states = torch.tensor(np.array(memory.states), dtype=torch.float32)\n",
    "        _, new_log_probs, new_values = agent.act(states)\n",
    "\n",
    "        # probability ratio\n",
    "        old_log_probs = torch.tensor(memory.log_probs, dtype=torch.float32).detach()\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        \n",
    "        # clipped surrogate loss\n",
    "        clipped_loss = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS)\n",
    "\n",
    "        # calculate policy loss and value loss\n",
    "        policy_loss = -torch.min(ratio * advantages, clipped_loss * advantages).mean()\n",
    "\n",
    "        returns = torch.tensor(np.array(returns))\n",
    "        value_loss = nn.MSELoss()(new_values.squeeze(), returns)\n",
    "        \n",
    "        # total loss\n",
    "        total_loss = policy_loss + 0.5 * value_loss\n",
    "\n",
    "        # backpropagate the loss and step the optimizer\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # TensorBoard Logging\n",
    "        writer.add_scalar(\"Loss/Policy Loss\", policy_loss.item(), episode_count)\n",
    "        writer.add_scalar(\"Loss/Value Loss\", value_loss.item(), episode_count)\n",
    "        writer.add_scalar(\"Loss/Total Loss\", total_loss.item(), episode_count)\n",
    "\n",
    "    # Log episode reward and length\n",
    "    writer.add_scalar(\"Episode Reward\", episode_reward, episode_count)\n",
    "    writer.add_scalar(\"Episode Length\", episode_length, episode_count)\n",
    "\n",
    "    if episode_count % 10 == 0:\n",
    "        print(f\"Episode {episode_count}, Reward: {episode_reward:.2f}, Steps: {episode_length}\")\n",
    "        print(f\"Action min: {min(memory.actions)}, Action max: {max(memory.actions)}, Action mean: {np.array(memory.actions).mean()}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a47c3381-07df-41d8-9c37-8e5a51c2e119",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# %tensorboard --logdir ./custom_ppo_logs/ --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07a25886-5416-4dee-bd51-3e0515b019ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Reward: -30.02, Steps: 999\n",
      "Action min: tensor([-0.9803]), Action max: tensor([0.9829]), Action mean: 0.0854717493057251\n",
      "Episode 10, Reward: -40.28, Steps: 999\n",
      "Action min: tensor([-0.9948]), Action max: tensor([0.9970]), Action mean: 0.034234922379255295\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m EPISODES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode_count \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPISODES):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[1;32m     12\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[9], line 80\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, agent, optimizer, num_steps, writer, episode_count, printit)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# backpropagate the loss and step the optimizer\u001b[39;00m\n\u001b[1;32m     79\u001b[0m total_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 80\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# TensorBoard Logging\u001b[39;00m\n\u001b[1;32m     83\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss/Policy Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, policy_loss\u001b[38;5;241m.\u001b[39mitem(), episode_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py:484\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    481\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 89\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py:226\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    214\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    216\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    217\u001b[0m         group,\n\u001b[1;32m    218\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    223\u001b[0m         state_steps,\n\u001b[1;32m    224\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py:161\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py:766\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    764\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 766\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py:422\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    418\u001b[0m bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstep\n\u001b[1;32m    420\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[0;32m--> 422\u001b[0m bias_correction2_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43m_dispatch_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_correction2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n\u001b[1;32m    426\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmaximum(max_exp_avg_sqs[i], exp_avg_sq, out\u001b[38;5;241m=\u001b[39mmax_exp_avg_sqs[i])\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py:114\u001b[0m, in \u001b[0;36m_dispatch_sqrt\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dispatch_sqrt\u001b[39m(\n\u001b[1;32m    115\u001b[0m     x: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    116\u001b[0m ):  \u001b[38;5;66;03m# float annotation is needed because of torchscript type inference\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39msqrt()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"rgb_array\", goal_velocity=0.1)\n",
    "env = RecordEpisodeStatistics(env)\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=\"./custom_ppo_logs\")\n",
    "\n",
    "# Training Loop \n",
    "EPISODES = 1000\n",
    "for episode_count in range(EPISODES):\n",
    "    train(env, agent, optimizer, 1000, writer, episode_count)\n",
    "\n",
    "# Cleanup\n",
    "env.close()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86a68bfa-855a-4b9b-b5b9-0f83762d301a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Reward: -97.92537157130785\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env_test = gym.make(\"MountainCarContinuous-v0\", render_mode=\"human\", goal_velocity=0.1)\n",
    "env_test = RecordEpisodeStatistics(env_test)\n",
    "\n",
    "state, _ = env_test.reset()\n",
    "total_reward = 0\n",
    "for _ in range(1000):\n",
    "    # action = agent.predict(state)\n",
    "    action, *rest = agent.act(state)\n",
    "    # print(state)\n",
    "    state, reward, terminated, truncated, _ = env_test.step(action)\n",
    "\n",
    "    total_reward += reward\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f'Episode Reward: {total_reward}')\n",
    "env_test.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a380e7eb-ec7a-4405-8b3f-96dd225df78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: [-0.44388816  0.        ]\n"
     ]
    }
   ],
   "source": [
    "state, _ = env.reset()\n",
    "print(f'State: {state}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9520c489-e150-4760-8797-1cce34a076e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5579fe-53cf-4013-81c2-1e83d7fc846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma, value = agent(torch.tensor(state))\n",
    "print(f'Mu: {mu}\\nSigma: {sigma}\\nvalue: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6100e62-2787-4839-8861-cc539da94d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "action, log_prob, value = agent.act(state)\n",
    "print(f'Action: {action}\\nLog Prob: {log_prob}\\nValue: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915a672-dfcf-4a05-abd1-5f9e2ff9cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Normal(mu, sigma)\n",
    "action = dist.sample()\n",
    "print(f'Action: {action}')\n",
    "print(f'Log Prob: {dist.log_prob(action).sum(axis=-1)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b15f28-0764-464c-9679-f21dfa492e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, reward, terminated, truncated, _ = env.step(action)\n",
    "print(f'State: {state}\\nReward: {reward}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6589926-71b3-4a4c-bb4b-33c8199e357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f1a46-dd08-4930-a9bb-584cd1b2b4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a4734-a043-4b99-aca6-9ddf5bc8a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.states.append(state)\n",
    "memory.actions.append(action)\n",
    "memory.log_probs.append(log_prob)\n",
    "memory.rewards.append(reward)\n",
    "memory.dones.append(done)\n",
    "memory.critic_values.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceab325-84fa-4b97-ac3d-28817913be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefaf533-4951-472a-9821-45b792c09e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_value = 0 if done else memory.critic_values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83438f37-8eb8-4fee-887b-2cc102be2ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "returns, advantages = compute_gae(\n",
    "            next_value=next_value,\n",
    "            rewards=memory.rewards,\n",
    "            dones=memory.dones,\n",
    "            values=memory.critic_values,\n",
    "            gamma=GAMMA,\n",
    "            lam=LAMBDA,\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3fb921-594a-438e-89fc-999d67bfdbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.array(memory.states)\n",
    "_, new_log_probs, new_values = agent.act(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d05c7-2822-46e1-a57a-0ac466cc5389",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb32fae-a7d1-4755-ab06-f674a58c21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_probs_detached = memory.log_probs.detach().numpy()\n",
    "ratio = torch.exp(new_log_probs.detach() - torch.tensor(memory.log_probs).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885539ff-df58-44b3-a453-0a8fef03435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c61c7d-bdd6-4d12-8cfe-9c9550433818",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = torch.tensor(np.array(returns))\n",
    "value_loss = nn.MSELoss()(new_values.squeeze(), returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feee271-2cee-40e8-af07-c188b106c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82961686-6def-4e52-a0ab-3ce47bb0886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clipped surrogate loss\n",
    "clipped_loss = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS)\n",
    "\n",
    "# calculate policy loss and value loss\n",
    "policy_loss = -torch.min(ratio * advantages, clipped_loss * advantages).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bce2db-09d9-447f-a59e-bef2075abe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = policy_loss + 0.5 * value_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f777b4bd-8c62-4f24-a808-183463a2eaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ffa471-1902-4fdd-9b55-2169e2f785f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in agent.named_parameters():\n",
    "    print(f\"{name}: requires_grad = {param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ca6ff-89f0-4ca8-be04-4d51e357d9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf14844-3ba3-4d99-b19e-e510bebb78fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = PPO(policy='MlpPolicy', env=env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97172bd5-8792-4d8e-af4c-9d3cbc8bb5bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# agent.learn(total_timesteps=int(1e+5), progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7af69-c75b-4b0a-a770-a6a8023194e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
