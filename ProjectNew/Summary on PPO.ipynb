{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d693dff-3da0-4c98-83e3-b71ddf06910a",
   "metadata": {},
   "source": [
    "# Summary on the \"[Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347v2)\" article by John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fca790-ba2e-4811-afbb-050525ba6e7b",
   "metadata": {},
   "source": [
    "## Introduction to the article\n",
    "\n",
    "In this article, the authors introduce a new group of reinforcement learning (RL) methods called Proximal Policy Optimization (PPO). \n",
    "The key idea behind PPO is to improve how RL agents learn from their environment. It works by:\n",
    "\n",
    "- Interacting with the Environment: The agent collects data by taking actions and observing the results, just like in traditional RL methods.\n",
    "- Optimizing with Multiple Updates: Unlike standard methods that update the agent’s policy after each new data sample, PPO allows for multiple updates using the same data, making learning more efficient.\n",
    "\n",
    "PPO is inspired by another algorithm called Trust Region Policy Optimization (TRPO). While TRPO is effective, it’s complex to implement. PPO offers similar benefits but is simpler, more flexible, and uses data more efficiently.\n",
    "\n",
    "**Goals of the Article**\n",
    "1. Introduce PPO: Explain how PPO works and what makes it different from other RL algorithms.\n",
    "2. Demonstrate PPO’s Efficiency: Show that PPO can learn faster and perform better with less data (better sample efficiency).\n",
    "3. Test Across Tasks: Evaluate PPO on different benchmark environments, such as robotic simulations and Atari games, to prove its versatility.\n",
    "4. Compare with Other Methods: Show that PPO outperforms other common RL algorithms in terms of performance, simplicity, and training time.\n",
    "\n",
    "In short, the article aims to present PPO as an effective, easy-to-use algorithm that balances strong performance with efficient learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4e11aa-5a4d-4edc-bcbd-64c78ee4e7d6",
   "metadata": {},
   "source": [
    "## What is PPO and how it works\n",
    "\n",
    "PPO (Proximal Policy Optimization) is based on policy gradient methods, which are a way for an **agent** (like a robot, autonomous vehicle, or game-playing AI) to learn how to make better decisions over time. The goal of the method is to improve the strategy of the agent, with the strategy being the way the agent chooses an action. This strategy is called the policy.\n",
    "\n",
    "By interacting with the environment, the agent obtains the results of its actions, called rewards, along with information on how the environment changes. After gathering the data—state, action, reward, and next state—the policy is then updated using gradient ascent methods. The formula used is:\n",
    "$$\\hat{g} = \\hat{E}_t \\left[ \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) \\hat{A}_t \\right]$$\n",
    "\n",
    "where $\\pi_{\\theta}(a_t | s_t)$ is the **policy** - telling the agent the probability of taking an action $a_t$ when it is in state $s_t$, \n",
    "\n",
    "$\\hat{A}_t$ is the advantage function, which compares how good an action is compared to the average action. If an action leads to a better outcome than expected, its advantage is high,\n",
    "\n",
    "$\\nabla_{\\theta}$ represents the gradient, calculating how much to change the policy to improve performance,\n",
    "\n",
    "$\\hat{E}_t$ is the empirical expectation, meaning the average outcome or expectation based on the data gathered.\n",
    "\n",
    "The loss of the objective function is calculated by differentiating the objective. However, performing multiple updates of the policy based on the loss from a single step is not recommended because it can lead to large policy updates, which can destabilize the training process.\n",
    "\n",
    "The article then explains the TRPO whis is a reinforcement learning algorithm designed to improve an agent’s policy while keeping updates stable and safe. The problem with regular policy gradient methods is that large updates to the policy can make learning unstable. TRPO solves this by restricting how much the policy can change in one update. It tries to maximize a special function called the **surrogate objective**, which estimates how good a new policy is compared to the old one. The goal is to improve the policy without making huge changes. This is done using the formula:\n",
    "$$\\begin{align}\n",
    "\\max_{\\theta} \\; & \\hat{E}_t \\left[ \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t \\right] \\tag{3} \\\\\n",
    "\\text{subject to} \\; & \\hat{E}_t \\left[ KL\\left( \\pi_{\\theta_{\\text{old}}}(\\cdot | s_t), \\pi_{\\theta}(\\cdot | s_t) \\right) \\right] \\leq \\delta \\tag{4}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\delta$ is small value that controls how much the policy is allowed to change. This is the **constrain**. It limit how much the new policy can differ from the old policy. This ensures that updates are small and stable, and\n",
    "\n",
    "KL: The Kullback-Leibler (KL) divergence, which measures the difference between the old and new policies, measuring the probability distributions for all actions rather than only one. The other option is to use **penalty**:\n",
    "$$\n",
    "\\max_{\\theta} \\; \\hat{E}_t \\left[ \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} \\hat{A}_t - \\beta \\, KL\\left( \\pi_{\\theta_{\\text{old}}}(\\cdot | s_t), \\pi_{\\theta}(\\cdot | s_t) \\right) \\right]\n",
    "$$\n",
    "\n",
    "where $\\beta$ is penalty "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78569e6-8fbd-4122-b6dd-b8485e24954c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
