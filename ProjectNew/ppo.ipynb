{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b02451-c710-4547-8cac-8f795470bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade grpcio grpcio-tools\n",
    "!pip install \"gymnasium[other]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8799ec51-d0be-4c4f-a4ad-4e8ad539f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir ./logs_new/ --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "605a33c7-17ed-42c1-9e6e-be50b0addb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-03 16:56:55.032678: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-03 16:56:55.042356: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738601815.054576   39169 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738601815.058371   39169 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-03 16:56:55.070861: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register, registry\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import time\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Any, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorboard\n",
    "\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6482d68a-fce7-40e6-8d5a-275862ba21e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'MarineEnv-v0' not in registry:\n",
    "    register(\n",
    "        id='MarineEnv-v0',\n",
    "        entry_point='environments:MarineEnv',  # String reference to the class\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1337d-61df-4e2c-ae20-98ec70f98b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_ipython = 'inline' in matplotlib.get_backend()\n",
    "# if is_ipython:\n",
    "#     from IPython import display\n",
    "\n",
    "# plt.ion()\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c63fc0-09d2-4f9c-98a3-6f4539521a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timescale = 1 / 3\n",
    "env_kwargs = dict(\n",
    "    render_mode='rgb_array',\n",
    "    continuous=True,\n",
    "    max_episode_steps=int(400 / timescale),\n",
    "    training_stage=2,\n",
    "    timescale=timescale,\n",
    "    training=True,\n",
    "    total_targets=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceee0548-ec75-47e1-afc1-09c26ce4ffe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /app/ProjectNew/video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "train_env = make_vec_env(env_id='MarineEnv-v0', n_envs=8, env_kwargs=env_kwargs)\n",
    "# eval_env = make_vec_env(env_id='MarineEnv-v0', n_envs=1, env_kwargs=env_kwargs)\n",
    "eval_env = gym.make('MarineEnv-v0', **env_kwargs)\n",
    "video_folder = './video/'\n",
    "trigger = lambda x: x % 2 == 0\n",
    "eval_env = RecordVideo(eval_env, video_folder, episode_trigger=trigger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3375503c-fcdc-4603-9365-e07a4e7bf91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear decay from 3e-4 to 1e-4\n",
    "initial_lr = 1e-3\n",
    "final_lr = 1e-4\n",
    "learning_rate_schedule = lambda progress_remaining: final_lr + (initial_lr - final_lr) * progress_remaining\n",
    "\n",
    "\n",
    "kwargs = {\n",
    "    'learning_rate': 0.0006440700528750355,\n",
    "    'n_steps': 2**10,\n",
    "    'batch_size': 2**10,\n",
    "    'gamma': 0.9595334803327593,\n",
    "    'gae_lambda': 0.9284293803518315,\n",
    "    'clip_range': 0.15247146447858756,\n",
    "    'ent_coef': 0.00017106771534852204,\n",
    "    'vf_coef': 0.8697801969581918,\n",
    "    'max_grad_norm': 1.1421017563147962,\n",
    "    'target_kl': 0.19795582328410327,\n",
    "    'n_epochs': 6,\n",
    "    \n",
    "    # 'clip_range': 0.2,  # Reduce to prevent large updates\n",
    "    # 'ent_coef': 5e-2,  # Higher entropy to encourage exploration\n",
    "    # 'gamma': 0.99, \n",
    "    # 'learning_rate': learning_rate_schedule,\n",
    "    # 'n_steps': 2048,  # Increase from default (512) to 2048\n",
    "    # 'batch_size': 512,  # Adjust batch size for stability\n",
    "    # 'gae_lambda': 0.95,  # Generalized Advantage Estimation smoothing\n",
    "    # 'max_grad_norm': 0.9, \n",
    "    'device': 'cpu',\n",
    "    'tensorboard_log': './logs_new/',\n",
    "    'policy_kwargs': {'net_arch': [128, 128], 'activation_fn': torch.nn.Tanh},  # Slightly deeper network\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9fc99d-3a67-45c1-b301-988c800a123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the evaluation callback\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path='./logs_new/best_model/',\n",
    "    log_path='./logs_new/results/',\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6413b289-fc9b-4df1-a806-7d1dc1a33333",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(\n",
    "    policy='MlpPolicy',\n",
    "    env=train_env,\n",
    "    verbose=0,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947167ce-ade1-4036-a3bf-356ae7b9f00a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e875be3b72b45b9915cd469192cffcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation \n",
       "environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and \n",
       "rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
       "  warnings.warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation \n",
       "environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and \n",
       "rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
       "  warnings.warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=40000, episode_reward=-141.14 +/- 843.71\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=40000, episode_reward=-141.14 +/- 843.71\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 163.20 +/- 62.85\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 163.20 +/- 62.85\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=80000, episode_reward=265.38 +/- 402.44\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=80000, episode_reward=265.38 +/- 402.44\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 178.80 +/- 85.94\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 178.80 +/- 85.94\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Eval num_timesteps=120000, episode_reward=759.07 +/- 230.83\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Eval num_timesteps=120000, episode_reward=759.07 +/- 230.83\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Episode length: 223.80 +/- 79.28\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Episode length: 223.80 +/- 79.28\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">New best mean reward!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "New best mean reward!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.learn(total_timesteps=(2e5), reset_num_timesteps=False, progress_bar=True, tb_log_name='ppo_9', callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0090d2c-324c-4945-bc3d-93fb25d43f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_agent = agent.load('./logs_new/best_model/best_model.zip', device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d533e-0a5d-4c2d-aaf5-467bad256d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = gym.make('MarineEnv-v0', **env_kwargs)\n",
    "mean, std = evaluate_policy(model=best_agent, env=eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f'Mean: {mean:.2f}, Std: {std:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcce986e-7b91-4254-a25c-42bc26d7954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save environment normalization stats\n",
    "# env.save(\"ppo_normalized_env.pkl\")\n",
    "# agent.save(\"ppo\")\n",
    "best_agent.save('ppo')\n",
    "# agent = agent.load(\"ppo\", device='cpu')\n",
    "# model = model.load('ppo_marine_stage_1.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e5042a-6ba5-4092-b202-5b02a635b376",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "env = VecNormalize.load(\"ppo_normalized_env.pkl\", env)\n",
    "\n",
    "# Disable reward normalization for evaluation\n",
    "env.training = False\n",
    "env.norm_reward = False\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "obs = env.reset()\n",
    "for _ in range(100):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, dones, _ = env.step(action)\n",
    "\n",
    "    # ✅ Ensure env.get_images() is not empty\n",
    "    images = env.get_images()\n",
    "    if images and images[0] is not None:\n",
    "        frame = images[0]\n",
    "        \n",
    "        # ✅ Ensure the frame has valid dimensions before displaying\n",
    "        if frame.shape[0] > 0 and frame.shape[1] > 0:\n",
    "            cv2.imshow(\"PPO MarineEnv Evaluation\", frame)\n",
    "            cv2.waitKey(1)  # Display for 1ms\n",
    "        else:\n",
    "            print(\"Warning: Received an empty frame from env.get_images()\")\n",
    "\n",
    "    if dones:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "cv2.destroyAllWindows()  # Close display window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1768e6-1c27-4e5b-9ffc-66d9743dad97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timescale = 1/6\n",
    "env_trn = gym.make('MarineEnv-v0', render_mode='human', continuous=True, training_stage=2, timescale=timescale, training=False, total_targets=3)\n",
    "for _ in range(5):\n",
    "    \n",
    "    state, _ = env_trn.reset()\n",
    "    print(f'Detected targets:', [target for target in env_trn.unwrapped.own_ship.detected_targets])\n",
    "    print(state)\n",
    "    episode_rewards = 0 \n",
    "    # flatten_state = flatten(env.observation_space, state)\n",
    "    # state = torch.tensor(flatten_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for _ in range(int(400 / timescale)):\n",
    "        action = best_agent.predict(state, deterministic=True)\n",
    "        # action = agent.predict(state, deterministic=True)\n",
    "        # print(action)\n",
    "        # observation, reward, terminated, truncated, info = env_trn.step((0, 0))\n",
    "        observation, reward, terminated, truncated, info = env_trn.step(action[0])\n",
    "        env_trn.render()\n",
    "        time.sleep(0.005)\n",
    "        episode_rewards += reward\n",
    "        print('===========================')\n",
    "        print(observation)\n",
    "        print(f'Step reward: {reward:.2f}')\n",
    "        print(f'Current Total reward: {episode_rewards:.2f}')\n",
    "        print(f'Dangerous targets: ', [target for target in env_trn.unwrapped.own_ship.dangerous_targets])\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            print('Episode total reward: ', episode_rewards)\n",
    "            print(info)\n",
    "            break\n",
    "    \n",
    "        state = observation\n",
    "            \n",
    "    print('Episode total rewards: ', episode_rewards)\n",
    "    print('Episode final state: ', state)\n",
    "    print(f'============================\\n' * 10)\n",
    "    env_trn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d50ff7-6e91-4359-a0a3-7bb1259dfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "# Create a folder to save videos\n",
    "video_folder = './video'\n",
    "os.makedirs(video_folder, exist_ok=True)\n",
    "\n",
    "env_trn = gym.make('MarineEnv-v0', render_mode='rgb_array', continuous=True, training_stage=2, timescale=timescale, training=False, total_targets=3)\n",
    "\n",
    "# Wrap the environment\n",
    "env = RecordVideo(env_trn, video_folder)\n",
    "\n",
    "# Run a single episode to record\n",
    "for _ in range(5):\n",
    "    state, _ = env.reset()\n",
    "    for _ in range(int(400 / timescale)):\n",
    "            action = best_agent.predict(state, deterministic=True)\n",
    "            observation, reward, terminated, truncated, info = env.step(action[0])\n",
    "            time.sleep(0.005)\n",
    "        \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "       \n",
    "            state = observation\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Locate the video file (the Monitor wrapper saves it with an .mp4 extension)\n",
    "import glob\n",
    "video_files = glob.glob(video_folder + \"/*.mp4\")\n",
    "print(\"Recorded video files:\", video_files)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fa9429-f316-40b6-8055-3df005a3a674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first recorded video (if available)\n",
    "if video_files:\n",
    "    video_file = video_files[0]\n",
    "    video = open(video_file, \"rb\").read()\n",
    "    video_b64 = b64encode(video).decode(\"utf-8\")\n",
    "    HTML(f\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"data:video/mp4;base64,{video_b64}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd7fa4-a2cd-4829-bc79-cf91c2735a21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
