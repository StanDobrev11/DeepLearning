{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4af1cab-73ee-458d-8ae0-ca050c98ffa2",
   "metadata": {},
   "source": [
    "# Creating Marine Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181518bf28e85ab8",
   "metadata": {},
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.envs.registration import register, registry\n",
    "import numpy as np\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd77b65221fa08d",
   "metadata": {},
   "source": [
    "if 'MarineEnv-v0' not in registry:\n",
    "    register(\n",
    "        id='MarineEnv-v0',\n",
    "        entry_point='environments:MarineEnv',  # String reference to the class\n",
    "    )\n",
    "\n",
    "# Test the environment\n",
    "env = gym.make('MarineEnv-v0', render_mode='rgb_array', continuous=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbc0218f078f6ad",
   "metadata": {},
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83dfb28b4b057409",
   "metadata": {},
   "source": [
    "Transition = namedtuple(typename='Transition', field_names=('state', 'action', 'next_state', 'reward'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd116a08b48339",
   "metadata": {},
   "source": [
    "class ReplayMemory:\n",
    "    \"\"\"\n",
    "    A replay buffer that stores transitions encountered by the agent.\n",
    "    It enables experience replay by sampling random batches of transitions\n",
    "    to break temporal correlation and improve learning stability.\n",
    "\n",
    "    Attributes:\n",
    "        capacity (int): The maximum number of transitions to store in memory.\n",
    "        memory (deque): A double-ended queue to store transitions with fixed size.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initializes the ReplayMemory with a fixed capacity.\n",
    "\n",
    "        Args:\n",
    "            capacity (int): The maximum number of transitions the buffer can hold.\n",
    "        \"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"\n",
    "        Add a transition to the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            *args: The elements of a transition (state, action, reward, next_state, done),\n",
    "                   which will be wrapped into a Transition namedtuple.\n",
    "        \"\"\"\n",
    "        state, action, next_state, reward = args\n",
    "        state = state.to(device)\n",
    "        action = action.to(device)\n",
    "        if next_state is not None:\n",
    "            next_state = next_state.to(device)\n",
    "        reward = reward.to(device)\n",
    "\n",
    "        self.memory.append(Transition(state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"\n",
    "        Randomly sample a batch of transitions from the memory buffer.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): The number of transitions to sample.\n",
    "\n",
    "        Returns:\n",
    "            List[Transition]: A list of randomly sampled transitions.\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the current size of the memory buffer.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of transitions currently stored in memory.\n",
    "        \"\"\"\n",
    "        return len(self.memory)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f67ce5fdf5e23596",
   "metadata": {},
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Deep Q-Network (DQN) implemented as a Multilayer Perceptron (MLP).\n",
    "\n",
    "    The network approximates the Q-value function, which predicts the cumulative \n",
    "    reward for each possible action in a given state.\n",
    "\n",
    "    Attributes:\n",
    "        layer1 (nn.Linear): First fully connected layer (input to 128 neurons).\n",
    "        layer2 (nn.Linear): Second fully connected layer (128 to 128 neurons).\n",
    "        layer3 (nn.Linear): Output layer (128 to n_actions neurons).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int):\n",
    "        \"\"\"\n",
    "        Initializes the DQN network with three fully connected layers.\n",
    "\n",
    "        Args:\n",
    "            n_observations (int): The size of the input (state of the environment).\n",
    "            n_actions (int): The number of possible actions in the environment.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)  # Input to hidden layer 1\n",
    "        self.layer2 = nn.Linear(128, 128)  # Hidden layer 1 to hidden layer 2\n",
    "        self.layer3 = nn.Linear(128, n_actions)  # Hidden layer 2 to output\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor representing the state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The Q-values for each possible action.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.layer1(x))  # Apply ReLU to the first layer\n",
    "        x = F.relu(self.layer2(x))  # Apply ReLU to the second layer\n",
    "        return self.layer3(x)  # Output layer (no activation applied)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0d9e53f75699072",
   "metadata": {},
   "source": [
    "# Size of the batch sampled from the replay buffer\n",
    "BATCH_SIZE = 128\n",
    "# Explanation:\n",
    "# The number of experiences (state-action-reward transitions) sampled from the replay buffer\n",
    "# at each training step. Larger batch sizes improve stability but increase computational cost.\n",
    "\n",
    "# Discount factor for future rewards\n",
    "GAMMA = 0.99\n",
    "# Explanation:\n",
    "# The discount factor determines how much importance is given to future rewards.\n",
    "# A value close to 1 means future rewards are highly valued, while a value closer to 0 means\n",
    "# the agent focuses on immediate rewards.\n",
    "\n",
    "# Initial value for epsilon in the epsilon-greedy policy\n",
    "EPS_START = 0.9\n",
    "# Explanation:\n",
    "# Epsilon is the probability of choosing a random action (exploration).\n",
    "# This is the starting value, meaning the agent initially explores 90% of the time.\n",
    "\n",
    "# Final value for epsilon in the epsilon-greedy policy\n",
    "EPS_END = 0.05\n",
    "# Explanation:\n",
    "# The minimum value epsilon can decay to. At this stage, the agent mostly exploits\n",
    "# the learned policy but still explores 5% of the time to avoid getting stuck in local optima.\n",
    "\n",
    "# Decay rate for epsilon in the epsilon-greedy policy\n",
    "EPS_DECAY = 1000\n",
    "# Explanation:\n",
    "# The rate at which epsilon decays over time. A higher value makes epsilon decay slower,\n",
    "# allowing the agent to explore for longer. The effective epsilon at time step `t` can\n",
    "# be calculated as:\n",
    "# epsilon(t) = EPS_END + (EPS_START - EPS_END) * exp(-t / EPS_DECAY)\n",
    "\n",
    "# Target network soft update rate\n",
    "TAU = 0.005\n",
    "# Explanation:\n",
    "# Determines how much the target network is updated towards the online (policy) network\n",
    "# at each training step. A lower TAU makes the target network update more slowly,\n",
    "# stabilizing the learning process. The update rule is:\n",
    "# target_param = TAU * policy_param + (1 - TAU) * target_param\n",
    "\n",
    "# Learning rate for the optimizer\n",
    "LR = 1e-04\n",
    "# Explanation:\n",
    "# The step size for gradient updates during backpropagation.\n",
    "# A smaller learning rate ensures slow and stable learning but may require more steps to converge.\n",
    "# Too large a value may lead to instability in training."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb0a4cfd21d8efb",
   "metadata": {},
   "source": [
    "# Number of actions in the environment\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "# Number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "# Initialize the policy and target networks\n",
    "policy_net = DQN(n_observations=n_observations, n_actions=n_actions).to(device)\n",
    "target_net = DQN(n_observations=n_observations, n_actions=n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())  # Copy weights from policy_net to target_net\n",
    "\n",
    "# Optimizer for training the policy network\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR)\n",
    "\n",
    "# Replay memory for experience replay\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "# Step counter for epsilon decay\n",
    "steps_done = 0\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eafee99f5da85578",
   "metadata": {},
   "source": [
    "def select_action(state: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Selects an action using an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        state (torch.Tensor): The current state of the environment.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The selected action as a 1x1 tensor (contains the action index).\n",
    "    \"\"\"\n",
    "    global steps_done\n",
    "    sample = random.random()  # Random number to determine explore vs exploit\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1.0 * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "\n",
    "    if sample > eps_threshold:  # Exploit: Use policy network to select the best action\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1).to(device)\n",
    "    else:  # Explore: Select a random action\n",
    "        return torch.tensor([[env.action_space.sample()]], dtype=torch.long, device=device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8299abfed6125eb7",
   "metadata": {},
   "source": [
    "episode_rewards = []  # To store cumulative rewards per episode\n",
    "\n",
    "\n",
    "def plot_rewards(show_result=False):\n",
    "    plt.figure(1)\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training')\n",
    "\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.plot(rewards_t.numpy(), label='Episode Reward')\n",
    "\n",
    "    # Plot moving average of the last 100 episodes\n",
    "    if len(rewards_t) >= 100:\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy(), label='100-Episode Average')\n",
    "\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58f23ad34b274568",
   "metadata": {},
   "source": [
    "def optimize_model():\n",
    "    \"\"\"\n",
    "    Perform one step of optimization for the policy network using the replay memory.\n",
    "\n",
    "    The function:\n",
    "        - Samples a batch of transitions from the replay memory.\n",
    "        - Computes the expected Q-values for the current state-action pairs.\n",
    "        - Computes the loss between the expected Q-values and the predicted Q-values.\n",
    "        - Performs a backward pass to update the policy network's parameters.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return  # Exit if there are not enough samples in memory to form a batch\n",
    "\n",
    "    # Sample a batch of transitions from the replay memory\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))  # Convert batch-array of transitions to Transition of batch-arrays\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool\n",
    "    )\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model's Q-values for the current states and actions\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Initialize tensor for the next state values\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "    # Compute expected Q-values\n",
    "    expected_state_action_values = reward_batch + (GAMMA * next_state_values)\n",
    "\n",
    "    # Compute Huber loss (Smooth L1 Loss)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()  # Clear the gradients from the previous step\n",
    "    loss.backward()  # Backpropagate the loss\n",
    "\n",
    "    # Clip gradients to avoid exploding gradients\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), clip_value=1.0)\n",
    "\n",
    "    # Perform a single optimization step\n",
    "    optimizer.step()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de9d254b348fa70a",
   "metadata": {},
   "source": [
    "def soft_update_target_network(policy_net, target_net, tau=0.01):\n",
    "    \"\"\"\n",
    "    Perform a soft update of the target network parameters.\n",
    "\n",
    "    Args:\n",
    "        policy_net (nn.Module): The policy network whose parameters are used for the update.\n",
    "        target_net (nn.Module): The target network to be updated.\n",
    "        tau (float): The soft update coefficient. Values closer to 1.0 mean faster updates.\n",
    "    \"\"\"\n",
    "    for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "        target_param.data.copy_(tau * policy_param.data + (1.0 - tau) * target_param.data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8e431ff27c95df8",
   "metadata": {},
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "training = True\n",
    "\n",
    "if training:\n",
    "    env.unwrapped.time_scale = 10\n",
    "    env.unwrapped.training = True\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    rewards = 0\n",
    "    \n",
    "    # for t in count():\n",
    "    for t in range(1000):\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        rewards += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # move to next state\n",
    "        state = next_state\n",
    "\n",
    "        # Update the model every N steps\n",
    "        # TRAIN_EVERY = 4\n",
    "        # if steps_done % TRAIN_EVERY == 0:\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update the target network\n",
    "        soft_update_target_network(policy_net, target_net, tau=TAU)\n",
    "\n",
    "        if done:\n",
    "            episode_rewards.append(rewards)\n",
    "            plot_rewards()\n",
    "            break\n",
    "\n",
    "print('Completed')\n",
    "plot_rewards(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f0f19a-48cf-44fb-a24f-6ed224bc5363",
   "metadata": {},
   "source": [
    "# torch.save(policy_net.state_dict(), 'dqn_policy_1.pth')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17507e72-2558-4040-87c2-a32dbbaa7813",
   "metadata": {},
   "source": [
    "policy_net.load_state_dict(torch.load(\"dqn_policy_1.pth\"))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aeb27cf672e3f1",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "env = gym.make('MarineEnv-v0')\n",
    "\n",
    "# Reset the environment\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "for t in range(10000):  # Maximum steps for visualization\n",
    "    with torch.no_grad():\n",
    "        # Use the trained policy network to select the best action\n",
    "        action = policy_net(state).max(1)[1].view(1, 1).item()\n",
    "        # action = env.action_space.sample()\n",
    "\n",
    "    # Take the action in the environment\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    # Render the environment\n",
    "    env.render()\n",
    "\n",
    "    # Prepare the next state\n",
    "    if not terminated:\n",
    "        state = torch.tensor(next_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    else:\n",
    "        break\n",
    "print(f\"Simulated Time: {info['total_sim_time']} hours\")\n",
    "env.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d1407-13eb-4efc-981e-ac26d528e91e",
   "metadata": {},
   "source": [
    "env.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dd8c7f-db86-46f4-ad3b-c6197b9f4f28",
   "metadata": {},
   "source": [
    "state"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dda22f-5fcf-442e-9eff-81d23d1e21d4",
   "metadata": {},
   "source": [
    "policy_net(state)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11ab12-d065-4592-b59d-f1a723d8f6cc",
   "metadata": {},
   "source": [
    "env.unwrapped.target_ship.cpa"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a32c7c-1426-4e3c-b980-9ab0c2b9683f",
   "metadata": {},
   "source": [
    "!python --version"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926f41e-6021-42d8-bbcd-02980cb7e6a4",
   "metadata": {},
   "source": [
    "!pip install stable-baselines3[extra]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d12f24-0ff2-42bb-b9cc-d0b9b146f3d6",
   "metadata": {},
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9dd6ab-17b0-443a-9c86-52968710ce36",
   "metadata": {},
   "source": [
    "sb_env = gym.make('MarineEnv-v0')\n",
    "\n",
    "training = True\n",
    "\n",
    "if training:\n",
    "    sb_env.unwrapped.time_scale = 10\n",
    "    sb_env.unwrapped.training = True\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f8aed-9e44-41a1-bab0-28eeb7795ede",
   "metadata": {},
   "source": [
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    sb_env,\n",
    "    verbose=1,\n",
    "    exploration_final_eps=0.1,\n",
    "    target_update_interval=250,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd376f-d60e-4ce0-b901-010550f0f55a",
   "metadata": {},
   "source": [
    "# Separate env for evaluation\n",
    "eval_env = gym.make(\"MarineEnv-v0\")\n",
    "eval_env.unwrapped.time_scale *= 10\n",
    "eval_env.unwrapped.training = True\n",
    "# Random Agent, before training\n",
    "mean_reward, std_reward = evaluate_policy(\n",
    "    model,\n",
    "    eval_env,\n",
    "    n_eval_episodes=10,\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820814a1-5ab1-4d43-b9e9-5e345edb6011",
   "metadata": {},
   "source": [
    "model.learn(total_timesteps=int(2e5), progress_bar=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e2d87-3f9e-471b-af9b-4959153f4b1a",
   "metadata": {},
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10cf57-6e28-466b-a1bc-8b42a012213f",
   "metadata": {},
   "source": [
    "vec_env = model.get_env()\n",
    "vec_env.unwrapped.time_scale = 1\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c98ed-981e-45e8-9169-b912b23cd9cb",
   "metadata": {},
   "source": [
    "obs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433dfff3-55ec-4cf7-947a-30643d000fab",
   "metadata": {},
   "source": [
    "model.predict(obs, deterministic=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42d00e4-ca9b-4ef0-a32f-2d0bcf11374e",
   "metadata": {},
   "source": [
    "obs, rewards, dones, info = vec_env.step(action)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90f955-5cb8-401c-a5d3-c0cc9ff73d21",
   "metadata": {},
   "source": [
    "rewards"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e35cd6-64d3-46e6-ade0-2bd01003d6bd",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
