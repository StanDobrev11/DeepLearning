{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22effdf0-f48b-4c7f-830e-16957abab865",
   "metadata": {},
   "source": [
    "!apt install swig cmake -y"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d305e8-3637-476f-8e47-d95fc969cebc",
   "metadata": {},
   "source": [
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7973ded-0e6f-4d6a-87d1-f38e237b038c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!apt-get update -y\n",
    "!apt-get install -y python3-opengl\n",
    "!apt install ffmpeg -y\n",
    "!apt install xvfb -y\n",
    "!pip3 install pyvirtualdisplay"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e3e3ae-770b-499f-9206-e2bdc548bb2e",
   "metadata": {},
   "source": [
    "import os\n",
    "os.kill(os.getpid(), 9)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e1e776-3929-4bb5-afab-005390c95834",
   "metadata": {},
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "# virtual_display.start()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4947ae5-5d1b-4fe2-9631-45a513d3f69f",
   "metadata": {},
   "source": [
    "# virtual_display.start()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "355abedd-9ccc-44e1-a92e-8f34adf71bed",
   "metadata": {},
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "\n",
    "from stable_baselines3 import PPO, DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cde3b1-2c02-49bd-b45a-057dddc8058d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "budget = 10_000\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Instantiate the agent\n",
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "# Train the agent and display a progress bar\n",
    "# model.learn(total_timesteps=budget, progress_bar=True)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934df64d-d37e-4193-b49b-96c7fdb80a31",
   "metadata": {},
   "source": [
    "# Save the agent\n",
    "model.save(\"dqn_lunar\")\n",
    "del model  # delete trained model to demonstrate loading\n",
    "\n",
    "# Load the trained agent\n",
    "# NOTE: if you have loading issue, you can pass `print_system_info=True`\n",
    "# to compare the system on which the model was trained vs the current one\n",
    "# model = DQN.load(\"dqn_lunar\", env=env, print_system_info=True)\n",
    "model = DQN.load(\"dqn_lunar\", env=env)\n",
    "\n",
    "# Evaluate the agent\n",
    "# NOTE: If you use wrappers with your environment that modify rewards,\n",
    "#       this will be reflected here. To evaluate with original rewards,\n",
    "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0e6b5-80f1-401f-90b2-2f8d91f82099",
   "metadata": {},
   "source": [
    "mean_reward, std_reward"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9661b68a-4fbc-49dc-ba5a-70a61fec4e4c",
   "metadata": {},
   "source": [
    "# Enjoy trained agent\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(10000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")\n",
    "vec_env.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037209f-addb-4a7b-ac90-8fd51f6e9775",
   "metadata": {},
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7690d5-c70b-415b-960f-7b0bebec6037",
   "metadata": {},
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "from typing import Any, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import plotly\n",
    "import tensorboard\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78da4f5a-af96-465c-b844-9dc1cea206de",
   "metadata": {},
   "source": [
    "%load_ext tensorboard"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7f7b7-eaba-4650-bd7b-a9cf27acf092",
   "metadata": {},
   "source": [
    "N_TRIALS = 100  # Maximum number of trials\n",
    "N_JOBS = 1 # Number of jobs to run in parallel\n",
    "N_STARTUP_TRIALS = 5  # Stop random sampling after N_STARTUP_TRIALS\n",
    "N_EVALUATIONS = 2  # Number of evaluations during the training\n",
    "N_TIMESTEPS = int(5e4)  # Training budget\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_ENVS = 10\n",
    "N_EVAL_EPISODES = 10\n",
    "TIMEOUT = int(60 * 15)  # 15 minutes\n",
    "\n",
    "ENV_ID = \"LunarLander-v2\"\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": ENV_ID,\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e46561f-84af-42bc-89fc-67284ddfa7f2",
   "metadata": {},
   "source": [
    "def sample_dqn_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1, log=True) # The learning rate, it can be a function of the current progress remaining (from 1 to 0)\n",
    "\n",
    "    buffer_size = 10 ** trial.suggest_int('buffer_size', 3, 7) # size of the replay buffer\n",
    "    \n",
    "    learning_starts =  2 ** trial.suggest_int('learning_starts', 3, 10) # how many steps of the model to collect transitions for before learning starts\n",
    "    \n",
    "    batch_size = 2 ** trial.suggest_int('batch_size', 3, 10) #  Minibatch size for each gradient update\n",
    "    \n",
    "    tau = trial.suggest_float('tau', 0.75, 1.0) # the soft update coefficient (“Polyak update”, between 0 and 1) default 1 for hard update\n",
    "    \n",
    "    gamma =  trial.suggest_float('gamma', 0.9, 0.9999) # the discount factor\n",
    "    \n",
    "    train_freq = trial.suggest_int('train_freq', 4, 200) # Update the model every train_freq steps. Alternatively pass a tuple of frequency and unit like (5, \"step\") or (2, \"episode\").\n",
    "    \n",
    "    gradient_steps = trial.suggest_int('gradient_steps', 1, 4) # How many gradient steps to do after each rollout (see train_freq) Set to -1 means to do as many gradient steps as steps done in the environment during the rollout.\n",
    "\n",
    "    target_update_interval = 10 ** trial.suggest_int('target_update_interval', 3, 7) # update the target network every target_update_interval environment steps.\n",
    "    \n",
    "    exploration_fraction = trial.suggest_float('exploration_fraction', 0.05, 0.5) # fraction of entire training period over which the exploration rate is reduced\n",
    "    \n",
    "    max_grad_norm = trial.suggest_float('max_grad_norm', 0.3, 5) # The maximum value for the gradient clipping\n",
    "            \n",
    "    net_arch = trial.suggest_categorical('net_arch', ['tiny', 'small'])\n",
    "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu'])\n",
    "\n",
    "    # display true values\n",
    "    trial.set_user_attr('gamma', gamma)\n",
    "\n",
    "    net_arch = [64] if net_arch == 'tiny' else [128, 128]\n",
    "    \n",
    "    activation_fn = {'tanh': nn.Tanh, 'relu': nn.ReLU}[activation_fn]\n",
    "    \n",
    "    return {\n",
    "        'exploration_fraction': exploration_fraction,\n",
    "        'target_update_interval': target_update_interval,\n",
    "        'gradient_steps': gradient_steps,\n",
    "        'train_freq': train_freq,\n",
    "        'tau': tau,\n",
    "        'batch_size': batch_size,\n",
    "        'buffer_size': buffer_size,\n",
    "        'gamma': gamma,\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_grad_norm': max_grad_norm,\n",
    "        'policy_kwargs': {\n",
    "            'net_arch': net_arch,\n",
    "            'activation_fn': activation_fn\n",
    "        }}\n",
    "    \n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cf3e9-0a54-42e8-8190-a37d06311e2b",
   "metadata": {},
   "source": [
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    Callback used for evaluating and reporting a trial.\n",
    "    \n",
    "    :param eval_env: Evaluation environement\n",
    "    :param trial: Optuna trial object\n",
    "    :param n_eval_episodes: Number of evaluation episodes\n",
    "    :param eval_freq:   Evaluate the agent every ``eval_freq`` call of the callback.\n",
    "    :param deterministic: Whether the evaluation should\n",
    "        use a stochastic or deterministic policy.\n",
    "    :param verbose:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Evaluate policy (done in the parent class)\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            # Send report to Optuna\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e031ef4-75ba-454f-be83-de74b8af6106",
   "metadata": {},
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function using by Optuna to evaluate\n",
    "    one configuration (i.e., one set of hyperparameters).\n",
    "\n",
    "    Given a trial object, it will sample hyperparameters,\n",
    "    evaluate it and report the result (mean episodic reward after training)\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: Mean episodic reward after training\n",
    "    \"\"\"\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "\n",
    "    # 1. Sample hyperparameters and update the keyword arguments\n",
    "    kwargs.update(**sample_dqn_params(trial))\n",
    "    print(kwargs)\n",
    "    # Create the RL model\n",
    "    model = DQN(**kwargs)\n",
    "    # Create eval envs\n",
    "    eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS, )\n",
    "\n",
    "    eval_callback = TrialEvalCallback(eval_envs, trial, N_EVAL_EPISODES, EVAL_FREQ, deterministic=True, verbose=0)\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        # Train the model\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback, progress_bar=True)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_envs.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c543f-0740-49a8-9fa7-f60167808ab6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Set pytorch num threads to 1 for faster training\n",
    "torch.set_num_threads(1)\n",
    "# Select the sampler, can be random, TPESampler, CMAES, ...\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(\n",
    "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3\n",
    ")\n",
    "# Create the study and start the hyperparameter optimization\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value: {trial.value}\")\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Write report\n",
    "study.trials_dataframe().to_csv(\"study_results_a2c_cartpole.csv\")\n",
    "\n",
    "fig1 = plot_optimization_history(study)\n",
    "fig2 = plot_param_importances(study)\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7de809d-e679-419d-b446-59744ecbd3df",
   "metadata": {},
   "source": [
    "trial.number"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57bd4bff-ecb1-4cc6-9782-1fb961d93edd",
   "metadata": {},
   "source": [
    "kwargs = {\n",
    "    'exploration_fraction': 0.7, \n",
    "    # 'target_update_interval': 10000, \n",
    "    # 'gradient_steps': 3, \n",
    "    # 'train_freq': 98, \n",
    "    'tau': 0.005, \n",
    "    'batch_size': 128, \n",
    "    'buffer_size': 10000, \n",
    "    'gamma': 0.99, \n",
    "    'learning_rate': 1e-4, \n",
    "    'max_grad_norm': 1.0, \n",
    "    'policy_kwargs': {'net_arch': [128, 128], 'activation_fn': nn.ReLU},\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f0eb1-7415-4259-849c-2cdc7399321b",
   "metadata": {},
   "source": [
    "model = DQN(**kwargs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938baef7-05e6-47ed-a220-bb59d017d6c9",
   "metadata": {},
   "source": [
    "best_kwargs = trial.params"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85572f3-c9f6-4fa2-b089-2ac15363351b",
   "metadata": {},
   "source": [
    "best_kwargs['activation_fn'] = nn.Tanh"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee7f88-b0d1-4c2e-b624-c4c7a971c12d",
   "metadata": {},
   "source": [
    "best_kwargs['net_arch'] = [64]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa5006-4f04-4c0d-b41b-c5d839c77a5d",
   "metadata": {},
   "source": [
    "best_kwargs['learning_rate'] = 1.0999765429841484e-05"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e126d2-c3be-4e37-9a5f-ad982af415a8",
   "metadata": {},
   "source": [
    "best_kwargs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1351c3-5a90-420a-abb0-971a7a926a84",
   "metadata": {},
   "source": [
    "del best_kwargs['lr']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f827151-cad2-44f2-afc8-3016233b3c88",
   "metadata": {},
   "source": [
    "policy_kwargs = {'net_arch': [64],\n",
    " 'activation_fn': torch.nn.modules.activation.Tanh,}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68682c57-6981-4b5a-89e8-3c36fcd77b71",
   "metadata": {},
   "source": [
    "best_kwargs['policy_kwargs'] = policy_kwargs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0ec77-93ad-4097-a068-87b7915bf823",
   "metadata": {},
   "source": [
    "del best_kwargs['net_arch']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0eb1d-bae4-4dcc-8f38-42524df90a10",
   "metadata": {},
   "source": [
    "del best_kwargs['activation_fn']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffed2db3-3a48-4450-a4bd-452dd6f0eed9",
   "metadata": {},
   "source": [
    "vec_env = make_vec_env('LunarLander-v2', n_envs=6)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "846d98e2-8299-4b34-b7d1-4ab5f770f5b5",
   "metadata": {},
   "source": [
    "try:\n",
    "    del test_model\n",
    "except NameError:\n",
    "    pass\n",
    "test_model = DQN('MlpPolicy', env=vec_env, **kwargs, verbose=0, tensorboard_log='./lunarlander_dqn_tensorboard_logs/')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c878480f-e265-4e7f-aebb-f801df5b8073",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "test_model.learn(total_timesteps=2_000_000, progress_bar=True, tb_log_name='final_run21')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f294cfd-5d39-4349-8ab9-78215e84e3ce",
   "metadata": {},
   "source": [
    "mean, std = evaluate_policy(model=test_model, env=vec_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f'Mean: {mean:.2f}, Std: {std:.2f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411ec91-b0da-48db-b340-5803bdafbff2",
   "metadata": {},
   "source": [
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def401b5-760b-4a2f-9f49-8d5f4b87392b",
   "metadata": {},
   "source": [
    "try:\n",
    "    del base_model\n",
    "except NameError:\n",
    "    pass\n",
    "    \n",
    "base_model = DQN('MlpPolicy', env=env, verbose=0, tensorboard_log='./lunarlander_dqn_tensorboard_logs/', **kwargs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc4e12-d1bf-475f-ab56-201d14fd5e3a",
   "metadata": {},
   "source": [
    "base_model.learn(total_timesteps=200_000, progress_bar=True, tb_log_name='first_run')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b78fb1c-a4a0-42b6-a9af-2e363fc36049",
   "metadata": {},
   "source": [
    "mean, std = evaluate_policy(model=base_model, env=env, n_eval_episodes=10, deterministic=True)\n",
    "print(f'Mean: {mean:.2f}, Std: {std:.2f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "175ec8a2-f1c3-4c98-8961-35ea6e6e23ea",
   "metadata": {},
   "source": [
    "%tensorboard --logdir ./lunarlander_dqn_tensorboard_logs/ --host=0.0.0.0"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de689038-f1b2-4161-8065-f45ab8fb9e75",
   "metadata": {},
   "source": [
    "vec_env = test_model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(10000):\n",
    "    action, _states = test_model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render(\"human\")\n",
    "vec_env.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d3173-d4d0-4a65-9abe-d9eb702fcc53",
   "metadata": {},
   "source": [
    "base_model.tau"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4905e-5c52-4ac2-88d2-e7298b0fdd08",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
