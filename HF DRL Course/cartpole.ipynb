{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20785c3d-7971-4057-aba9-f792ce986d25",
   "metadata": {},
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f07fab8f-056e-4ea6-a963-711fb2abb3ad",
   "metadata": {},
   "source": [
    "!pip install plotly"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbeef5eb-a75e-4ba2-abc1-fd7e5c206fac",
   "metadata": {},
   "source": [
    "vec_env = make_vec_env(\"CartPole-v1\", n_envs=4)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495b4af3-d30e-4b84-88dc-e6cdd21e0e90",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "budget = 20_000\n",
    "\n",
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\", verbose=1, seed=8, device='cpu')\n",
    "model.learn(total_timesteps=budget, progress_bar=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "927314d4-97b9-4bcd-b377-976b9e3d2d2a",
   "metadata": {},
   "source": [
    "mean, std = evaluate_policy(model, vec_env, n_eval_episodes=50, deterministic=True)\n",
    "print(f'Mean reward = {mean:.3f}, Std of reward = +/-{std:.3f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a63a5f9-f450-498b-b9d2-90af0887a3a6",
   "metadata": {},
   "source": [
    "policy_kwargs = dict(\n",
    "    net_arch=[\n",
    "        dict(vf=[64, 64], pi=[64, 64])\n",
    "    ],\n",
    "    activation_fn=nn.Tanh,\n",
    ")\n",
    "\n",
    "hyperparameters = dict(\n",
    "    n_steps=1024,\n",
    "    learning_rate=1e-3,\n",
    "    gamma=0.99,\n",
    "    max_grad_norm=1,\n",
    "    ent_coef=0.0001,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5e88ef-62a7-4f21-b891-e62ea2c001d6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model_tuned = A2C(\"MlpPolicy\", \"CartPole-v1\", verbose=1, **hyperparameters, device='cpu').learn(total_timesteps=budget, progress_bar=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31d21770-6837-4d79-8e4c-929ca2d2d7b9",
   "metadata": {},
   "source": [
    "vec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "mean, std = evaluate_policy(model_tuned, vec_env, n_eval_episodes=50, deterministic=True)\n",
    "print(f'Mean reward = {mean:.3f}, Std of reward = +/-{std:.3f}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60f98054-c0fc-41b1-9520-3dc605fd520f",
   "metadata": {},
   "source": [
    "# obs = vec_env.reset()\n",
    "# while True:\n",
    "#     action, _states = model_tuned.predict(obs)\n",
    "#     obs, rewards, dones, info = vec_env.step(action)\n",
    "#     vec_env.render(\"human\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b863744b-d4e4-4f8c-aa5b-379eb6f15487",
   "metadata": {},
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "from typing import Any, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "549c581d-8e61-4558-936b-7df148de0a5b",
   "metadata": {},
   "source": [
    "N_TRIALS = 100  # Maximum number of trials\n",
    "N_JOBS = 1 # Number of jobs to run in parallel\n",
    "N_STARTUP_TRIALS = 5  # Stop random sampling after N_STARTUP_TRIALS\n",
    "N_EVALUATIONS = 2  # Number of evaluations during the training\n",
    "N_TIMESTEPS = int(2e4)  # Training budget\n",
    "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
    "N_EVAL_ENVS = 5\n",
    "N_EVAL_EPISODES = 10\n",
    "TIMEOUT = int(60 * 15)  # 15 minutes\n",
    "\n",
    "ENV_ID = \"CartPole-v1\"\n",
    "\n",
    "DEFAULT_HYPERPARAMS = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": ENV_ID,\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb0572e5-d4a8-459a-ba6d-8dece5913d29",
   "metadata": {},
   "source": [
    "def sample_a2c_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
    "    # discount between 0.9 and 0.9999\n",
    "    gamma = 1.0 - trial.suggest_float('gamma', 0.0001, 0.1, log=True)\n",
    "\n",
    "    max_grad_norm = trial.suggest_float('max_grad_norm', 0.3, 5.0, log=True)\n",
    "\n",
    "    n_steps = 2 ** trial.suggest_int('exponent_n_steps', 3, 10)\n",
    "\n",
    "    # learning rate between 1e-5, 1\n",
    "    learning_rate = trial.suggest_float('lr', 1e-5, 1, log=True)\n",
    "    net_arch = trial.suggest_categorical('net_arch', ['tiny', 'small'])\n",
    "    activation_fn = trial.suggest_categorical('activation_fn', ['tanh', 'relu'])\n",
    "\n",
    "    # display true values\n",
    "    trial.set_user_attr('gamma', gamma)\n",
    "    trial.set_user_attr('n_steps', n_steps)\n",
    "\n",
    "    net_arch = {'pi': [64], 'vf': [64]} if net_arch == 'tiny' else {'pi': [64, 64], 'vf': [64, 64]}\n",
    "    activation_fn = {'tanh': nn.Tanh, 'relu': nn.ReLU}[activation_fn]\n",
    "    \n",
    "    return {\n",
    "        'n_steps': n_steps,\n",
    "        'gamma': gamma,\n",
    "        'learning_rate': learning_rate,\n",
    "        'max_grad_norm': max_grad_norm,\n",
    "        'policy_kwargs': {\n",
    "            'net_arch': net_arch,\n",
    "            'activation_fn': activation_fn\n",
    "        }}\n",
    "    \n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e15b4c8-be06-4da5-a03a-57d06d29a9f1",
   "metadata": {},
   "source": [
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    Callback used for evaluating and reporting a trial.\n",
    "    \n",
    "    :param eval_env: Evaluation environement\n",
    "    :param trial: Optuna trial object\n",
    "    :param n_eval_episodes: Number of evaluation episodes\n",
    "    :param eval_freq:   Evaluate the agent every ``eval_freq`` call of the callback.\n",
    "    :param deterministic: Whether the evaluation should\n",
    "        use a stochastic or deterministic policy.\n",
    "    :param verbose:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Evaluate policy (done in the parent class)\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            # Send report to Optuna\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7c6888e-398e-41ff-a7da-259948ceb7b0",
   "metadata": {},
   "source": [
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    Objective function using by Optuna to evaluate\n",
    "    one configuration (i.e., one set of hyperparameters).\n",
    "\n",
    "    Given a trial object, it will sample hyperparameters,\n",
    "    evaluate it and report the result (mean episodic reward after training)\n",
    "\n",
    "    :param trial: Optuna trial object\n",
    "    :return: Mean episodic reward after training\n",
    "    \"\"\"\n",
    "\n",
    "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
    "    ### YOUR CODE HERE\n",
    "    # TODO: \n",
    "    # 1. Sample hyperparameters and update the default keyword arguments: `kwargs.update(other_params)`\n",
    "    # 2. Create the evaluation envs\n",
    "    # 3. Create the `TrialEvalCallback`\n",
    "\n",
    "    # 1. Sample hyperparameters and update the keyword arguments\n",
    "    kwargs.update(**sample_a2c_params(trial))\n",
    "    \n",
    "    # Create the RL model\n",
    "    model = A2C(**kwargs, device='cpu')\n",
    "\n",
    "    # 2. Create envs used for evaluation using `make_vec_env`, `ENV_ID` and `N_EVAL_ENVS`\n",
    "    eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS, )\n",
    "    # 3. Create the `TrialEvalCallback` callback defined above that will periodically evaluate\n",
    "    # and report the performance using `N_EVAL_EPISODES` every `EVAL_FREQ`\n",
    "    # TrialEvalCallback signature:\n",
    "    # TrialEvalCallback(eval_env, trial, n_eval_episodes, eval_freq, deterministic, verbose)\n",
    "    eval_callback = TrialEvalCallback(eval_envs, trial, N_EVAL_EPISODES, EVAL_FREQ, deterministic=True, verbose=0)\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        # Train the model\n",
    "        model.learn(N_TIMESTEPS, callback=eval_callback, progress_bar=True)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_envs.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d149db1-2239-46bd-a814-4ba428e8e5a5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import torch as th\n",
    "\n",
    "# Set pytorch num threads to 1 for faster training\n",
    "th.set_num_threads(1)\n",
    "# Select the sampler, can be random, TPESampler, CMAES, ...\n",
    "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
    "# Do not prune before 1/3 of the max budget is used\n",
    "pruner = MedianPruner(\n",
    "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3\n",
    ")\n",
    "# Create the study and start the hyperparameter optimization\n",
    "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
    "\n",
    "try:\n",
    "    study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value: {trial.value}\")\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Write report\n",
    "study.trials_dataframe().to_csv(\"study_results_a2c_cartpole.csv\")\n",
    "\n",
    "fig1 = plot_optimization_history(study)\n",
    "fig2 = plot_param_importances(study)\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4778df2d-01b3-4df5-9abe-b7cf77ef802c",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import plotly\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d8d5e7b-867e-4a47-8047-36acc68f9ad4",
   "metadata": {},
   "source": [
    "study = pd.read_csv('study_results_a2c_cartpole.csv', index_col=0)\n",
    "\n",
    "# fig1 = plt.hist(study)\n",
    "# fig2 = plt.hist(study, bins='auto')\n",
    "\n",
    "# fig1.show()\n",
    "# fig2.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06d929f9-e810-4201-8e4d-799350f63462",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "study[:10]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e5189a1-e078-4134-bffd-5e0b01d4117b",
   "metadata": {},
   "source": [
    "model.policy"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bddcba-18e0-4f38-adf0-4cd02a9d8b1e",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
